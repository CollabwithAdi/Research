{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 0: Set Global Seed for Reproducibility\nimport random\nimport numpy as np\nimport torch\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Install essential libraries for training, evaluation, and model analysis\n!pip install torch torchvision\n!pip install timm transformers roboflow pycocotools\n!pip install grad-cam torchinfo\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Connect to Roboflow and download the COCO-format segmentation dataset\nfrom roboflow import Roboflow\nimport os\nimport json\n\nrf = Roboflow(api_key=\"Mvt9FCxE4mY6vBy5OG08\")  # Replace with your key if needed\nproject = rf.workspace(\"urban-lake-wastef\").project(\"another_approach_try\")\nversion = project.version(4)\ndataset = version.download(\"coco-segmentation\")\n\n# Set paths to the annotation file and images\nDATA_ROOT = dataset.location\nTRAIN_JSON = os.path.join(DATA_ROOT, 'train', '_annotations.coco.json')\nIMG_DIR = os.path.join(DATA_ROOT, 'train')\n\n# Define output directories\nREGION_ROOT = '/content/extracted_regions'\nSPLIT_ROOT = '/content/split_regions'\nos.makedirs(REGION_ROOT, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Extract foreground objects from COCO masks and save cropped images\n\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\n# Load COCO annotation JSON\nwith open(TRAIN_JSON) as f:\n    ann_data = json.load(f)\n\n# Create a mapping from category ID to name\ncat_map = {c['id']: c['name'] for c in ann_data['categories']}\n\n# Iterate over all annotations to extract and save cropped object regions\nfor ann in ann_data['annotations']:\n    img_info = next(img for img in ann_data['images'] if img['id'] == ann['image_id'])\n    img_path = os.path.join(IMG_DIR, img_info['file_name'])\n    img = Image.open(img_path).convert('RGB')\n\n    seg = ann['segmentation']\n    mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n\n    for poly in seg:\n        pts = np.array(poly).reshape(-1, 2)\n        m = Image.new('L', (img_info['width'], img_info['height']), 0)\n        ImageDraw.Draw(m).polygon([tuple(p) for p in pts], outline=1, fill=1)\n        mask = np.maximum(mask, np.array(m))\n\n    if mask.sum() < 100:  # Skip very small masks\n        continue\n\n    region = np.array(img) * mask[:, :, None]\n    region_img = Image.fromarray(region)\n\n    label = cat_map[ann['category_id']]\n    out_dir = os.path.join(REGION_ROOT, label)\n    os.makedirs(out_dir, exist_ok=True)\n    base = os.path.splitext(img_info['file_name'])[0]\n    out_path = os.path.join(out_dir, f\"{base}_{ann['id']}.png\")\n    region_img.save(out_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Visualize 10 original vs masked (extracted) regions ‚Äì before training\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport random\n\ndef show_before_after_masked(original_dir, masked_dir, split_name, num_samples=10):\n    \"\"\"\n    Displays side-by-side original vs masked images from a dataset split.\n    \"\"\"\n    print(f\"\\nüîç Showing {num_samples} {split_name} images: original vs masked\")\n\n    classes = sorted(os.listdir(masked_dir))\n    selected_images = []\n\n    while len(selected_images) < num_samples:\n        chosen_class = random.choice(classes)\n        class_mask_dir = os.path.join(masked_dir, chosen_class)\n        if not os.path.isdir(class_mask_dir):\n            continue\n        mask_files = os.listdir(class_mask_dir)\n        if not mask_files:\n            continue\n\n        chosen_file = random.choice(mask_files)\n        selected_images.append((chosen_class, chosen_file))\n\n    for class_name, file_name in selected_images:\n        masked_path = os.path.join(masked_dir, class_name, file_name)\n\n        # Extract original filename base\n        original_basename_base = \"_\".join(file_name.split(\"_\")[:-1])\n        possible_exts = [\".jpg\", \".png\"]\n        original_path = None\n\n        for ext in possible_exts:\n            candidate = os.path.join(original_dir, original_basename_base + ext)\n            if os.path.exists(candidate):\n                original_path = candidate\n                break\n\n        if not original_path:\n            print(f\"‚ö†Ô∏è Original not found for: {original_basename_base}\")\n            continue\n\n        original_img = Image.open(original_path).convert(\"RGB\")\n        masked_img = Image.open(masked_path).convert(\"RGB\")\n\n        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n        axs[0].imshow(original_img)\n        axs[0].set_title(\"Original\")\n        axs[0].axis(\"off\")\n\n        axs[1].imshow(masked_img)\n        axs[1].set_title(\"Masked (Extracted)\")\n        axs[1].axis(\"off\")\n\n        fig.suptitle(f\"üü¢ Class: {class_name} | üìÑ File: {file_name}\", fontsize=13)\n        plt.tight_layout()\n        plt.show()\n\n# ‚úÖ Call this after Step 3\nshow_before_after_masked(\n    original_dir=os.path.join(DATA_ROOT, \"train\"),\n    masked_dir=\"/content/extracted_regions\",\n    split_name=\"Train Set\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4.5: Split dataset into 60% train, 20% val, 20% test\n\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Create split folders\nfor split in ['train', 'val', 'test']:\n    os.makedirs(os.path.join(SPLIT_ROOT, split), exist_ok=True)\n\n# For each class, split its images into train/val/test\nfor class_name in os.listdir(REGION_ROOT):\n    class_path = os.path.join(REGION_ROOT, class_name)\n    if not os.path.isdir(class_path):\n        continue\n\n    files = os.listdir(class_path)\n    train_files, temp_files = train_test_split(files, test_size=0.4, random_state=42)\n    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n\n    for split, split_files in zip(['train', 'val', 'test'], [train_files, val_files, test_files]):\n        split_class_dir = os.path.join(SPLIT_ROOT, split, class_name)\n        os.makedirs(split_class_dir, exist_ok=True)\n        for f in split_files:\n            shutil.copy2(os.path.join(class_path, f), os.path.join(split_class_dir, f))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Define a PyTorch Dataset class with augmentation and weighted sampling\n\nfrom torch.utils.data import Dataset, WeightedRandomSampler\nfrom torchvision import transforms\nfrom collections import defaultdict\nfrom PIL import Image\n\nclass WasteRegionDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.samples = []\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.03),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        self.label2id = {}\n        class_counts = defaultdict(int)\n\n        # Collect all image paths and assign class indices\n        classes = sorted(os.listdir(root_dir))\n        self.label2id = {c: i for i, c in enumerate(classes)}\n\n        for c in classes:\n            class_dir = os.path.join(root_dir, c)\n            for f in os.listdir(class_dir):\n                self.samples.append((os.path.join(class_dir, f), self.label2id[c]))\n                class_counts[self.label2id[c]] += 1\n\n        # Compute sample weights for balancing\n        self.sample_weights = [1.0 / class_counts[label] for _, label in self.samples]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        img = Image.open(path).convert('RGB')\n        img = self.transform(img)\n        return img, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Set device (GPU or CPU) and define paths to the dataset splits\nfrom timm import create_model\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Define the root directories for each data split\nTRAIN_DIR = os.path.join(SPLIT_ROOT, 'train')\nVAL_DIR   = os.path.join(SPLIT_ROOT, 'val')\nTEST_DIR  = os.path.join(SPLIT_ROOT, 'test')\n\n# Determine the number of classes from folder names in train set\nnum_classes = len(os.listdir(TRAIN_DIR))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Initialize datasets and DataLoaders with augmentation and weighted sampling for training\n\n# Create dataset instances\ntrain_ds = WasteRegionDataset(TRAIN_DIR)\nval_ds   = WasteRegionDataset(VAL_DIR, transform=train_ds.transform)\ntest_ds  = WasteRegionDataset(TEST_DIR, transform=val_ds.transform)\n\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\n# Apply WeightedRandomSampler to address class imbalance in training data\ntrain_sampler = WeightedRandomSampler(train_ds.sample_weights, len(train_ds.sample_weights), replacement=True)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler)\nval_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)\ntest_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Define training function with mixup, label smoothing, early stopping, weighted loss, and checkpoint saving\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.cuda.amp import GradScaler\nimport torch.nn as nn\n\ndef train_one_model(model_name, use_mixup=True, label_smooth=0.1, seed=42, patience=15, save_as=None):\n    set_seed(seed)\n    model = create_model(model_name, pretrained=True, num_classes=num_classes).to(device)\n\n    labels = [label for _, label in train_ds.samples]\n    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smooth)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, num_warmup_steps=3 * len(train_loader),\n        num_training_steps=len(train_loader) * 100\n    )\n    scaler = GradScaler()\n\n    def mixup_data(x, y, alpha=0.4):\n        lam = np.random.beta(alpha, alpha)\n        index = torch.randperm(x.size(0)).to(x.device)\n        mixed_x = lam * x + (1 - lam) * x[index]\n        y_a, y_b = y, y[index]\n        return mixed_x, y_a, y_b, lam\n\n    def mixup_criterion(criterion, pred, y_a, y_b, lam):\n        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'train_acc': []}\n    best_val_loss = float('inf')\n    no_improve_epochs = 0\n    save_path = f\"{save_as}_best.pth\" if save_as else f\"{model_name}_best.pth\"\n\n    for epoch in range(2):\n        model.train()\n        total_loss = 0\n        train_correct = train_total = 0\n\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            if use_mixup:\n                imgs, y_a, y_b, lam = mixup_data(imgs, labels)\n            else:\n                y_a, y_b, lam = labels, labels, 1.0\n\n            with torch.cuda.amp.autocast():\n                outputs = model(imgs)\n                loss = mixup_criterion(criterion, outputs, y_a, y_b, lam) if use_mixup else criterion(outputs, labels)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n\n            # Calculate training accuracy (only for non-mixup, for mixup it's not meaningful)\n            if not use_mixup:\n                preds = outputs.argmax(dim=1)\n                train_correct += (preds == labels).sum().item()\n                train_total += labels.size(0)\n\n        scheduler.step()\n        avg_train_loss = total_loss / len(train_loader)\n        history['train_loss'].append(avg_train_loss)\n\n        # Compute training accuracy for this epoch\n        if not use_mixup:\n            train_acc = train_correct / train_total if train_total > 0 else 0.0\n        else:\n            train_acc = None  # Or np.nan\n\n        history['train_acc'].append(train_acc)\n\n        model.eval()\n        val_loss = 0\n        correct = total = 0\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs, labels = imgs.to(device), labels.to(device)\n                outputs = model(imgs)\n                val_loss += criterion(outputs, labels).item()\n                preds = outputs.argmax(dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n\n        avg_val_loss = val_loss / len(val_loader)\n        val_acc = correct / total\n        history['val_loss'].append(avg_val_loss)\n        history['val_acc'].append(val_acc)\n\n        print(f\"[{model_name}] Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | \"\n              f\"Val Loss: {avg_val_loss:.4f} | \"\n              f\"Val Acc: {val_acc:.4f}\" +\n              (f\" | Train Acc: {train_acc:.4f}\" if train_acc is not None else \"\"))\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            no_improve_epochs = 0\n            torch.save(model.state_dict(), save_path)\n            print(\"üî∏ New best model saved.\")\n        else:\n            no_improve_epochs += 1\n            if no_improve_epochs >= patience:\n                print(\"üõë Early stopping triggered.\")\n                break\n\n    return model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Train multiple configurations of model and save each model uniquely\nimport csv\n\nmodel_list = ['efficientnet_b0']\nseeds = [42, 123, 777]\nmixup_options = [True, False]\nlabel_smoothings = [0.1, 0.0]\n\nall_histories = {}\n\nwith open(\"ablation_results.csv\", \"w\", newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow([\"Model\", \"Seed\", \"Mixup\", \"Label Smoothing\", \"Val Accuracy\", \"Checkpoint\"])\n\n    for model_name in model_list:\n        for mixup in mixup_options:\n            for smooth in label_smoothings:\n                for seed in seeds:\n                    run_id = f\"{model_name}_mixup{mixup}_smooth{smooth}_seed{seed}\"\n                    print(f\"üîÅ Training: {run_id}\")\n\n                    model, history = train_one_model(\n                        model_name=model_name,\n                        use_mixup=mixup,\n                        label_smooth=smooth,\n                        seed=seed,\n                        patience=15,\n                        save_as=run_id\n                    )\n\n                    final_val_acc = history['val_acc'][-1]\n                    writer.writerow([model_name, seed, mixup, smooth, final_val_acc, f\"{run_id}_best.pth\"])\n                    all_histories[run_id] = history\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Plot training vs validation loss and accuracy to assess learning and overfitting\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_model_histories(all_histories):\n    \"\"\"\n    Plots loss and accuracy curves from the training history of each model configuration.\n    \n    Parameters:\n    - all_histories (dict): Dictionary of training histories returned by train_one_model().\n                            Keys are run names, values are dicts with 'train_loss', 'val_loss', 'val_acc', 'train_acc'.\n\n    Each model will generate:\n    - Train vs Val Loss curve\n    - Train vs Val Accuracy curve\n    \"\"\"\n    for run_id, hist in all_histories.items():\n        epochs = range(1, len(hist['train_loss']) + 1)\n        \n        plt.figure(figsize=(16, 5))\n        \n        # Loss Curve\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, hist['train_loss'], label='Train Loss', marker='o')\n        plt.plot(epochs, hist['val_loss'], label='Val Loss', marker='x')\n        plt.title(f\"{run_id} ‚Äì Loss Curve\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.grid(True)\n\n        # Accuracy Curve\n        plt.subplot(1, 2, 2)\n        # Only plot train accuracy if it exists and has at least one non-None value\n        if 'train_acc' in hist and any(x is not None for x in hist['train_acc']):\n            # Replace None with np.nan for plotting\n            train_acc_plot = [x if x is not None else np.nan for x in hist['train_acc']]\n            plt.plot(epochs, train_acc_plot, label='Train Accuracy', marker='d', color='blue')\n        plt.plot(epochs, hist['val_acc'], label='Val Accuracy', marker='s', color='green')\n        plt.title(f\"{run_id} ‚Äì Accuracy Curve\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.grid(True)\n\n        plt.suptitle(f\"Training Progress ‚Äì {run_id}\", fontsize=14)\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 11: Generate overfitting plots for all trained models using saved training histories\nplot_model_histories(all_histories)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 12: Read the CSV results and compute average and std deviation for each model setting\nimport pandas as pd\n\ndf = pd.read_csv(\"ablation_results.csv\")\n\n# Print the raw ablation results table\nprint(\"\\nüìù All ablation results:\\n\")\nprint(df)\n\n# Compute summary statistics\nsummary = df.groupby([\"Model\", \"Mixup\", \"Label Smoothing\"])[\"Val Accuracy\"].agg(['mean', 'std']).reset_index()\n\n# Print the summary table\nprint(\"\\nüìä Ablation Results (mean ¬± std across seeds):\\n\")\nprint(summary)\n\n# Find and print the best configuration\nbest_row = df.loc[df[\"Val Accuracy\"].idxmax()]\nprint(\"\\n\\nüèÜ Best configuration:\\n\\n\", best_row)\n\n# Extract values for next steps if needed\nbest_checkpoint_path = best_row[\"Checkpoint\"]\nbest_model_name = best_row[\"Model\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# STEP 13: Evaluate Best-Performing Model on Held-Out Test Set\n# ============================================================\n# This step loads the best model configuration (based on validation accuracy)\n# and evaluates it on the test dataset using accuracy, precision, recall,\n# F1 score, confusion matrix, and ROC AUC. All model parameters used in training\n# (architecture, mixup, label smoothing, seed) are printed for traceability.\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    classification_report, confusion_matrix, roc_auc_score\n)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import label_binarize\n\n# üîé Show model configuration\nprint(\"\\nüß™ STEP 13: Evaluating Best Model Configuration on TEST SET\\n\")\nprint(f\"Model Architecture: {best_row['Model']}\")\nprint(f\"Mixup Enabled:      {best_row['Mixup']}\")\nprint(f\"Label Smoothing:    {best_row['Label Smoothing']}\")\nprint(f\"Random Seed:        {best_row['Seed']}\")\nprint(f\"Validation Accuracy:{best_row['Val Accuracy']:.4f}\")\nprint(f\"Checkpoint File:    {best_row['Checkpoint']}\")\nprint(f\"‚û°Ô∏è Dataset Used:      Held-out TEST SET\\n\")\n\n# üîß Load best-performing model\nmodel = create_model(best_model_name, pretrained=False, num_classes=num_classes).to(device)\nmodel.load_state_dict(torch.load(best_checkpoint_path))\n\ndef evaluate_model_detailed(model, dataloader, device, label2id, show_roc_auc=True):\n    model.eval()\n    all_preds, all_probs, all_labels = [], [], []\n\n    with torch.no_grad():\n        for imgs, labels in dataloader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            preds = outputs.argmax(dim=1)\n\n            all_probs.extend(probs.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n    all_probs = np.array(all_probs)\n\n    id2label = {v: k for k, v in label2id.items()}\n    target_names = [id2label[i] for i in sorted(id2label)]\n\n    print(\"\\nüîç Classification Report (on Test Set):\\n\")\n    print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n    print(f\"Test Accuracy:  {accuracy_score(all_labels, all_preds):.4f}\")\n    print(f\"Test Precision: {precision_score(all_labels, all_preds, average='weighted'):.4f}\")\n    print(f\"Test Recall:    {recall_score(all_labels, all_preds, average='weighted'):.4f}\")\n    print(f\"Test F1-Score:  {f1_score(all_labels, all_preds, average='weighted'):.4f}\")\n\n    # Confusion Matrix ‚Äì Raw\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=target_names, yticklabels=target_names)\n    plt.title(\"Confusion Matrix ‚Äì Raw (Test Set)\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    plt.show()\n\n    # Confusion Matrix ‚Äì Normalized\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Oranges',\n                xticklabels=target_names, yticklabels=target_names)\n    plt.title(\"Confusion Matrix ‚Äì Normalized (Test Set)\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    plt.show()\n\n    # ROC AUC Score\n    if show_roc_auc:\n        try:\n            y_true_bin = label_binarize(all_labels, classes=list(range(len(label2id))))\n            auc = roc_auc_score(y_true_bin, all_probs, multi_class='ovr')\n            print(f\"Test ROC AUC (Multiclass OVR): {auc:.4f}\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è ROC-AUC computation failed: {e}\")\n\n# üß™ Evaluate best model on test data\nevaluate_model_detailed(model, test_loader, device, train_ds.label2id)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}