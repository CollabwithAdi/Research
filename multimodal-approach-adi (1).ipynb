{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 0: Set Global Seed for Reproducibility\nimport random\nimport numpy as np\nimport torch\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Install essential libraries for training, evaluation, and model analysis\n!pip install torch torchvision\n!pip install timm transformers roboflow pycocotools\n!pip install grad-cam torchinfo\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Connect to Roboflow and download the COCO-format segmentation dataset\nfrom roboflow import Roboflow\nimport os\nimport json\n\nrf = Roboflow(api_key=\"Mvt9FCxE4mY6vBy5OG08\")  # Replace with your key if needed\nproject = rf.workspace(\"urban-lake-wastef\").project(\"another_approach_try\")\nversion = project.version(4)\ndataset = version.download(\"coco-segmentation\")\n\n# Set paths to the annotation file and images\nDATA_ROOT = dataset.location\nTRAIN_JSON = os.path.join(DATA_ROOT, 'train', '_annotations.coco.json')\nIMG_DIR = os.path.join(DATA_ROOT, 'train')\n\n# Define output directories\nREGION_ROOT = '/content/extracted_regions'\nSPLIT_ROOT = '/content/split_regions'\nos.makedirs(REGION_ROOT, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Extract foreground objects from COCO masks and save cropped images\n\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\n# Load COCO annotation JSON\nwith open(TRAIN_JSON) as f:\n    ann_data = json.load(f)\n\n# Create a mapping from category ID to name\ncat_map = {c['id']: c['name'] for c in ann_data['categories']}\n\n# Iterate over all annotations to extract and save cropped object regions\nfor ann in ann_data['annotations']:\n    img_info = next(img for img in ann_data['images'] if img['id'] == ann['image_id'])\n    img_path = os.path.join(IMG_DIR, img_info['file_name'])\n    img = Image.open(img_path).convert('RGB')\n\n    seg = ann['segmentation']\n    mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n\n    for poly in seg:\n        pts = np.array(poly).reshape(-1, 2)\n        m = Image.new('L', (img_info['width'], img_info['height']), 0)\n        ImageDraw.Draw(m).polygon([tuple(p) for p in pts], outline=1, fill=1)\n        mask = np.maximum(mask, np.array(m))\n\n    if mask.sum() < 100:  # Skip very small masks\n        continue\n\n    region = np.array(img) * mask[:, :, None]\n    region_img = Image.fromarray(region)\n\n    label = cat_map[ann['category_id']]\n    out_dir = os.path.join(REGION_ROOT, label)\n    os.makedirs(out_dir, exist_ok=True)\n    base = os.path.splitext(img_info['file_name'])[0]\n    out_path = os.path.join(out_dir, f\"{base}_{ann['id']}.png\")\n    region_img.save(out_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Split dataset into 60% train, 20% val, 20% test\n\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Create split folders\nfor split in ['train', 'val', 'test']:\n    os.makedirs(os.path.join(SPLIT_ROOT, split), exist_ok=True)\n\n# For each class, split its images into train/val/test\nfor class_name in os.listdir(REGION_ROOT):\n    class_path = os.path.join(REGION_ROOT, class_name)\n    if not os.path.isdir(class_path):\n        continue\n\n    files = os.listdir(class_path)\n    train_files, temp_files = train_test_split(files, test_size=0.4, random_state=42)\n    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n\n    for split, split_files in zip(['train', 'val', 'test'], [train_files, val_files, test_files]):\n        split_class_dir = os.path.join(SPLIT_ROOT, split, class_name)\n        os.makedirs(split_class_dir, exist_ok=True)\n        for f in split_files:\n            shutil.copy2(os.path.join(class_path, f), os.path.join(split_class_dir, f))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Define a PyTorch Dataset class with augmentation and weighted sampling\n\nfrom torch.utils.data import Dataset, WeightedRandomSampler\nfrom torchvision import transforms\nfrom collections import defaultdict\nfrom PIL import Image\n\nclass WasteRegionDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.samples = []\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.03),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        self.label2id = {}\n        class_counts = defaultdict(int)\n\n        # Collect all image paths and assign class indices\n        classes = sorted(os.listdir(root_dir))\n        self.label2id = {c: i for i, c in enumerate(classes)}\n\n        for c in classes:\n            class_dir = os.path.join(root_dir, c)\n            for f in os.listdir(class_dir):\n                self.samples.append((os.path.join(class_dir, f), self.label2id[c]))\n                class_counts[self.label2id[c]] += 1\n\n        # Compute sample weights for balancing\n        self.sample_weights = [1.0 / class_counts[label] for _, label in self.samples]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        img = Image.open(path).convert('RGB')\n        img = self.transform(img)\n        return img, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Set device (GPU or CPU) and define paths to the dataset splits\nfrom timm import create_model\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Define the root directories for each data split\nTRAIN_DIR = os.path.join(SPLIT_ROOT, 'train')\nVAL_DIR   = os.path.join(SPLIT_ROOT, 'val')\nTEST_DIR  = os.path.join(SPLIT_ROOT, 'test')\n\n# Determine the number of classes from folder names in train set\nnum_classes = len(os.listdir(TRAIN_DIR))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Initialize datasets and DataLoaders with augmentation and weighted sampling for training\n\n# Create dataset instances\ntrain_ds = WasteRegionDataset(TRAIN_DIR)\nval_ds   = WasteRegionDataset(VAL_DIR, transform=train_ds.transform)\ntest_ds  = WasteRegionDataset(TEST_DIR, transform=val_ds.transform)\n\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\n# Apply WeightedRandomSampler to address class imbalance in training data\ntrain_sampler = WeightedRandomSampler(train_ds.sample_weights, len(train_ds.sample_weights), replacement=True)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler)\nval_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)\ntest_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Define training function with:\n# - Mixup augmentation\n# - Label smoothing\n# - Early stopping with patience\n# - Weighted class loss\n# - Saving best model by validation loss\n\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.cuda.amp import GradScaler\nimport torch.nn as nn\n\ndef train_one_model(model_name, use_mixup=True, label_smooth=0.1, seed=42, patience=10):\n    set_seed(seed)\n    model = create_model(model_name, pretrained=True, num_classes=num_classes).to(device)\n\n    # Compute class weights for loss function\n    labels = [label for _, label in train_ds.samples]\n    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n    # Define loss function, optimizer, scheduler, and mixed precision scaler\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smooth)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, num_warmup_steps=3 * len(train_loader),\n        num_training_steps=len(train_loader) * 100\n    )\n    scaler = GradScaler()\n\n    # Define Mixup helpers\n    def mixup_data(x, y, alpha=0.4):\n        lam = np.random.beta(alpha, alpha)\n        index = torch.randperm(x.size(0)).to(x.device)\n        mixed_x = lam * x + (1 - lam) * x[index]\n        y_a, y_b = y, y[index]\n        return mixed_x, y_a, y_b, lam\n\n    def mixup_criterion(criterion, pred, y_a, y_b, lam):\n        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n    # Initialize training state\n    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n    best_val_loss = float('inf')\n    no_improve_epochs = 0\n\n    # Training loop\n    for epoch in range(200):\n        model.train()\n        total_loss = 0\n\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            if use_mixup:\n                imgs, y_a, y_b, lam = mixup_data(imgs, labels)\n            else:\n                y_a, y_b, lam = labels, labels, 1.0\n\n            with torch.cuda.amp.autocast():\n                outputs = model(imgs)\n                loss = mixup_criterion(criterion, outputs, y_a, y_b, lam) if use_mixup else criterion(outputs, labels)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n\n        scheduler.step()\n        avg_train_loss = total_loss / len(train_loader)\n        history['train_loss'].append(avg_train_loss)\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        correct = total = 0\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs, labels = imgs.to(device), labels.to(device)\n                outputs = model(imgs)\n                val_loss += criterion(outputs, labels).item()\n                preds = outputs.argmax(dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n\n        avg_val_loss = val_loss / len(val_loader)\n        val_acc = correct / total\n        history['val_loss'].append(avg_val_loss)\n        history['val_acc'].append(val_acc)\n\n        print(f\"[{model_name}] Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n        # Save best model\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            no_improve_epochs = 0\n            torch.save(model.state_dict(), f\"{model_name}_best.pth\")\n            print(\"ðŸ”¸ New best model saved.\")\n        else:\n            no_improve_epochs += 1\n            if no_improve_epochs >= patience:\n                print(\"ðŸ›‘ Early stopping triggered.\")\n                break\n\n    return model, history\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Train multiple models using different settings (mixup, label smoothing, seeds)\n# Log validation accuracy to a CSV file and collect training history for visualization\n\nimport csv\n\nmodel_list = ['efficientnet_b0', 'resnet34', 'densenet121', 'convnext_tiny', 'swin_tiny_patch4_window7_224']\nseeds = [42, 123, 777]\nmixup_options = [True, False]\nlabel_smoothings = [0.1, 0.0]\n\nall_histories = {}  # Store histories to later visualize training curves\n\nwith open(\"ablation_results.csv\", \"w\", newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow([\"Model\", \"Seed\", \"Mixup\", \"Label Smoothing\", \"Val Accuracy\"])\n    \n    for model_name in model_list:\n        for mixup in mixup_options:\n            for smooth in label_smoothings:\n                for seed in seeds:\n                    run_id = f\"{model_name}_mixup{mixup}_smooth{smooth}_seed{seed}\"\n                    print(f\"ðŸ” Training: {run_id}\")\n                    \n                    model, history = train_one_model(\n                        model_name, use_mixup=mixup, label_smooth=smooth, seed=seed\n                    )\n\n                    # Save final validation accuracy to CSV\n                    final_val_acc = history['val_acc'][-1]\n                    writer.writerow([model_name, seed, mixup, smooth, final_val_acc])\n                    \n                    # Save training history to plot later\n                    all_histories[run_id] = history\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10.1: Plot training vs validation loss and validation accuracy to assess learning and overfitting\n\nimport matplotlib.pyplot as plt\n\ndef plot_model_histories(all_histories):\n    \"\"\"\n    Plots loss and accuracy curves from the training history of each model configuration.\n    \n    Parameters:\n    - all_histories (dict): Dictionary of training histories returned by train_one_model().\n                            Keys are run names, values are dicts with 'train_loss', 'val_loss', 'val_acc'.\n\n    Each model will generate:\n    - Train vs Val Loss curve\n    - Val Accuracy curve\n    \"\"\"\n    for run_id, hist in all_histories.items():\n        epochs = range(1, len(hist['train_loss']) + 1)\n        \n        plt.figure(figsize=(14, 5))\n        \n        # Loss Curve\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, hist['train_loss'], label='Train Loss', marker='o')\n        plt.plot(epochs, hist['val_loss'], label='Val Loss', marker='x')\n        plt.title(f\"{run_id} â€“ Loss Curve\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.grid(True)\n\n        # Accuracy Curve\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, hist['val_acc'], label='Val Accuracy', marker='s', color='green')\n        plt.title(f\"{run_id} â€“ Accuracy Curve\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.grid(True)\n\n        plt.suptitle(f\"Training Progress â€“ {run_id}\", fontsize=14)\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10.2: Generate overfitting plots for all trained models using saved training histories\nplot_model_histories(all_histories)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Read the CSV results and compute average and std deviation for each model setting\nimport pandas as pd\n\n# Load the ablation results\ndf = pd.read_csv(\"ablation_results.csv\")\n\n# Group by configuration and calculate mean Â± std\nsummary = df.groupby([\"Model\", \"Mixup\", \"Label Smoothing\"])[\"Val Accuracy\"].agg(['mean', 'std']).reset_index()\n\n# Print summary table\nprint(\"ðŸ“Š Ablation Results (mean Â± std across seeds):\")\nprint(summary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 11: Evaluate best-performing model on held-out test set\n# Includes confusion matrix, classification report, per-class F1, precision, recall, ROC-AUC\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    classification_report, confusion_matrix, roc_auc_score,\n    precision_recall_fscore_support\n)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import label_binarize\n\ndef evaluate_model_detailed(model, dataloader, device, label2id, show_roc_auc=True):\n    model.eval()\n    all_preds, all_probs, all_labels = [], [], []\n\n    # Collect predictions and ground truth\n    with torch.no_grad():\n        for imgs, labels in dataloader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            preds = outputs.argmax(dim=1)\n\n            all_probs.extend(probs.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n    all_probs = np.array(all_probs)\n\n    # Create label mapping\n    id2label = {v: k for k, v in label2id.items()}\n    target_names = [id2label[i] for i in sorted(id2label)]\n\n    # Print classification metrics\n    print(\"ðŸ” Classification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n    print(f\"Accuracy:  {accuracy_score(all_labels, all_preds):.4f}\")\n    print(f\"Precision: {precision_score(all_labels, all_preds, average='weighted'):.4f}\")\n    print(f\"Recall:    {recall_score(all_labels, all_preds, average='weighted'):.4f}\")\n    print(f\"F1-Score:  {f1_score(all_labels, all_preds, average='weighted'):.4f}\")\n\n    # Plot confusion matrix (raw)\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=target_names, yticklabels=target_names)\n    plt.title(\"Confusion Matrix â€“ Raw\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    plt.show()\n\n    # Normalized confusion matrix\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Oranges',\n                xticklabels=target_names, yticklabels=target_names)\n    plt.title(\"Confusion Matrix â€“ Normalized\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    plt.show()\n\n    # ROC AUC (optional)\n    if show_roc_auc:\n        try:\n            y_true_bin = label_binarize(all_labels, classes=list(range(len(label2id))))\n            auc = roc_auc_score(y_true_bin, all_probs, multi_class='ovr')\n            print(f\"ROC AUC (Multiclass OVR): {auc:.4f}\")\n        except Exception as e:\n            print(f\"âš ï¸ ROC-AUC computation failed: {e}\")\n\n# Load best model (adjust name if needed)\nbest_model_name = 'efficientnet_b0'  # Replace with best performing model if needed\nmodel = create_model(best_model_name, pretrained=False, num_classes=num_classes).to(device)\nmodel.load_state_dict(torch.load(f\"{best_model_name}_best.pth\"))\n\n# Evaluate on test data\nevaluate_model_detailed(model, test_loader, device, train_ds.label2id)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 12: Visualize model attention (Grad-CAM) for a given input image and target class\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef apply_gradcam(model, image_path, class_idx, target_layer=None):\n    model.eval()\n\n    # Choose last transformer block or final convolutional layer\n    if target_layer is None:\n        target_layer = [model.blocks[-1].norm1] if hasattr(model, 'blocks') else [model.layer4[-1]]\n\n    # Initialize Grad-CAM\n    cam = GradCAM(model=model, target_layers=target_layer, use_cuda=torch.cuda.is_available())\n\n    # Load and preprocess image\n    img = Image.open(image_path).convert('RGB').resize((224, 224))\n    rgb_img = np.array(img) / 255.0\n    input_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])(img).unsqueeze(0).to(device)\n\n    # Generate heatmap\n    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(class_idx)])[0]\n    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n\n    # Show the result\n    plt.imshow(cam_image)\n    plt.title(f\"Grad-CAM for class index {class_idx}\")\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 13: Visualize the model's learned feature space using t-SNE dimensionality reduction\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\ndef plot_tsne_features(model, dataloader, device, label2id, num_samples=500):\n    model.eval()\n    features = []\n    labels = []\n\n    # Extract features and labels\n    with torch.no_grad():\n        for imgs, lbls in dataloader:\n            imgs = imgs.to(device)\n            feats = model.forward_features(imgs) if hasattr(model, 'forward_features') else model(imgs)\n            features.append(feats.cpu().numpy())\n            labels.extend(lbls.numpy())\n            if len(labels) >= num_samples:\n                break\n\n    # Prepare for t-SNE\n    features = np.concatenate(features, axis=0)[:num_samples]\n    labels = labels[:num_samples]\n\n    # Run t-SNE\n    tsne = TSNE(n_components=2, random_state=42)\n    tsne_feats = tsne.fit_transform(features)\n    label_names = [list(label2id.keys())[i] for i in labels]\n\n    # Plot t-SNE\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=tsne_feats[:, 0], y=tsne_feats[:, 1], hue=label_names, palette=\"deep\", alpha=0.7)\n    plt.title(\"t-SNE of Learned Features\")\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 14: Plot Precision-Recall curve per class to assess class-specific confidence\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.preprocessing import label_binarize\n\ndef plot_precision_recall(model, dataloader, device, label2id):\n    model.eval()\n    all_labels = []\n    all_probs = []\n\n    # Gather probabilities and labels\n    with torch.no_grad():\n        for imgs, labels in dataloader:\n            imgs = imgs.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.numpy())\n\n    # Format data for multi-class PR calculation\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n    n_classes = len(label2id)\n    y_true = label_binarize(all_labels, classes=list(range(n_classes)))\n\n    # Plot PR curve for each class\n    plt.figure(figsize=(10, 6))\n    for i in range(n_classes):\n        precision, recall, _ = precision_recall_curve(y_true[:, i], all_probs[:, i])\n        ap = average_precision_score(y_true[:, i], all_probs[:, i])\n        plt.plot(recall, precision, label=f\"Class {i} (AP={ap:.2f})\")\n\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(\"Precision-Recall Curve (per class)\")\n    plt.legend()\n    plt.grid()\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 15: Print model architecture summary including parameter count and MACs (FLOPs)\nfrom torchinfo import summary\n\ndef print_model_summary(model, input_size=(1, 3, 224, 224)):\n    model.eval()\n    print(summary(model, input_size=input_size, col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}