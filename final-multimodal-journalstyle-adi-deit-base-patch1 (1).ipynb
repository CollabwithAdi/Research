{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 0: Create output folders for all stages, including interpretability results\n\nimport os\n\nKAGGLE_WORKING = '/kaggle/working'\nOUTPUT_ROOT = os.path.join(KAGGLE_WORKING, \"outputs\")\nFEATURES_DIR = os.path.join(OUTPUT_ROOT, \"features\")           # for extracted regions/crops\nSPLITS_DIR = os.path.join(OUTPUT_ROOT, \"splits\")               # for train/val/test splits\nMODELS_DIR = os.path.join(OUTPUT_ROOT, \"models\")               # trained models\nLOGS_DIR = os.path.join(OUTPUT_ROOT, \"logs\")                   # training logs, CSVs\nMETRICS_DIR = os.path.join(OUTPUT_ROOT, \"metrics\")             # evaluation metrics as JSON/CSV\nVIS_DIR = os.path.join(OUTPUT_ROOT, \"visualizations\")          # plots, confusion matrix, etc.\nBEST_MODEL_DIR = os.path.join(OUTPUT_ROOT, \"best_model_results\") # graphs and evaluations for best model\nINTERP_DIR = os.path.join(OUTPUT_ROOT, \"interpretability\")     # interpretability visualizations\nSTATS_DIR = os.path.join(OUTPUT_ROOT, \"stats\")                 # statistical significance test results\n\nfor d in [\n    OUTPUT_ROOT, FEATURES_DIR, SPLITS_DIR, MODELS_DIR, LOGS_DIR,\n    METRICS_DIR, VIS_DIR, BEST_MODEL_DIR, INTERP_DIR, STATS_DIR\n]:\n    os.makedirs(d, exist_ok=True)\n\nTRAIN_DIR = os.path.join(SPLITS_DIR, \"train\")\nVAL_DIR = os.path.join(SPLITS_DIR, \"val\")\nTEST_DIR = os.path.join(SPLITS_DIR, \"test\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Set Global Seed for Reproducibility\n\nimport random\nimport numpy as np\nimport torch\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Install essential libraries for training, evaluation, and model analysis (Kaggle: may skip if preinstalled)\n!pip install torch torchvision\n!pip install timm transformers roboflow pycocotools\n!pip install grad-cam torchinfo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Download the COCO-format segmentation dataset from Roboflow\n\nfrom roboflow import Roboflow\nimport os\nimport json\n\nrf = Roboflow(api_key=\"Mvt9FCxE4mY6vBy5OG08\")  # Replace with your key if needed\nproject = rf.workspace(\"urban-lake-wastef\").project(\"another_approach_try\")\nversion = project.version(4)\ndataset = version.download(\"coco-segmentation\")\n\nDATA_ROOT = dataset.location\nTRAIN_JSON = os.path.join(DATA_ROOT, 'train', '_annotations.coco.json')\nIMG_DIR = os.path.join(DATA_ROOT, 'train')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Extract foreground objects from COCO masks and save cropped images in FEATURES_DIR\n\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\nwith open(TRAIN_JSON) as f:\n    ann_data = json.load(f)\n\ncat_map = {c['id']: c['name'] for c in ann_data['categories']}\n\nfor ann in ann_data['annotations']:\n    img_info = next(img for img in ann_data['images'] if img['id'] == ann['image_id'])\n    img_path = os.path.join(IMG_DIR, img_info['file_name'])\n    img = Image.open(img_path).convert('RGB')\n\n    seg = ann['segmentation']\n    mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n\n    for poly in seg:\n        pts = np.array(poly).reshape(-1, 2)\n        m = Image.new('L', (img_info['width'], img_info['height']), 0)\n        ImageDraw.Draw(m).polygon([tuple(p) for p in pts], outline=1, fill=1)\n        mask = np.maximum(mask, np.array(m))\n\n    if mask.sum() < 100:  # Skip very small masks\n        continue\n\n    region = np.array(img) * mask[:, :, None]\n    region_img = Image.fromarray(region)\n\n    label = cat_map[ann['category_id']]\n    out_dir = os.path.join(FEATURES_DIR, label)\n    os.makedirs(out_dir, exist_ok=True)\n    base = os.path.splitext(img_info['file_name'])[0]\n    out_path = os.path.join(out_dir, f\"{base}_{ann['id']}.png\")\n    region_img.save(out_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Visualize 10 original vs masked crops for journal-quality reporting and save in VIS_DIR\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport random\n\ndef show_before_after_masked(original_dir, masked_dir, split_name, num_samples=10, vis_dir=None):\n    print(f\"\\n🔍 Showing {num_samples} {split_name} images: original vs masked\")\n    classes = sorted(os.listdir(masked_dir))\n    selected_images = []\n    while len(selected_images) < num_samples:\n        chosen_class = random.choice(classes)\n        class_mask_dir = os.path.join(masked_dir, chosen_class)\n        if not os.path.isdir(class_mask_dir):\n            continue\n        mask_files = os.listdir(class_mask_dir)\n        if not mask_files:\n            continue\n        chosen_file = random.choice(mask_files)\n        selected_images.append((chosen_class, chosen_file))\n    for idx, (class_name, file_name) in enumerate(selected_images):\n        masked_path = os.path.join(masked_dir, class_name, file_name)\n        original_basename_base = \"_\".join(file_name.split(\"_\")[:-1])\n        possible_exts = [\".jpg\", \".png\"]\n        original_path = None\n        for ext in possible_exts:\n            candidate = os.path.join(original_dir, original_basename_base + ext)\n            if os.path.exists(candidate):\n                original_path = candidate\n                break\n        if not original_path:\n            print(f\"⚠️ Original not found for: {original_basename_base}\")\n            continue\n        original_img = Image.open(original_path).convert(\"RGB\")\n        masked_img = Image.open(masked_path).convert(\"RGB\")\n        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n        axs[0].imshow(original_img)\n        axs[0].set_title(\"Original\")\n        axs[0].axis(\"off\")\n        axs[1].imshow(masked_img)\n        axs[1].set_title(\"Masked (Extracted)\")\n        axs[1].axis(\"off\")\n        fig.suptitle(f\"🟢 Class: {class_name} | 📄 File: {file_name}\", fontsize=13)\n        plt.tight_layout()\n        if vis_dir:\n            vis_path = os.path.join(vis_dir, f\"show_before_after_{split_name}_{idx+1}.png\")\n            plt.savefig(vis_path)\n        plt.show()\n\nshow_before_after_masked(\n    original_dir=os.path.join(DATA_ROOT, \"train\"),\n    masked_dir=FEATURES_DIR,\n    split_name=\"Train Set\",\n    vis_dir=VIS_DIR\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Split dataset into 60% train, 20% val, 20% test in SPLITS_DIR\n\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\nfor split in ['train', 'val', 'test']:\n    os.makedirs(os.path.join(SPLITS_DIR, split), exist_ok=True)\n\nfor class_name in os.listdir(FEATURES_DIR):\n    class_path = os.path.join(FEATURES_DIR, class_name)\n    if not os.path.isdir(class_path):\n        continue\n    files = os.listdir(class_path)\n    train_files, temp_files = train_test_split(files, test_size=0.4, random_state=42)\n    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n    for split, split_files in zip(['train', 'val', 'test'], [train_files, val_files, test_files]):\n        split_class_dir = os.path.join(SPLITS_DIR, split, class_name)\n        os.makedirs(split_class_dir, exist_ok=True)\n        for f in split_files:\n            shutil.copy2(os.path.join(class_path, f), os.path.join(split_class_dir, f))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Define a PyTorch Dataset class with augmentation and weighted sampling\n\nfrom torch.utils.data import Dataset, WeightedRandomSampler\nfrom torchvision import transforms\nfrom collections import defaultdict\nfrom PIL import Image\n\nclass WasteRegionDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.samples = []\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.03),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        self.label2id = {}\n        class_counts = defaultdict(int)\n        classes = sorted(os.listdir(root_dir))\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        for c in classes:\n            class_dir = os.path.join(root_dir, c)\n            for f in os.listdir(class_dir):\n                self.samples.append((os.path.join(class_dir, f), self.label2id[c]))\n                class_counts[self.label2id[c]] += 1\n        self.sample_weights = [1.0 / class_counts[label] for _, label in self.samples]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        img = Image.open(path).convert('RGB')\n        img = self.transform(img)\n        return img, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Set device and initialize DataLoaders with journal-quality split paths\n\nfrom timm import create_model\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nnum_classes = len(os.listdir(TRAIN_DIR))\n\ntrain_ds = WasteRegionDataset(TRAIN_DIR)\nval_ds   = WasteRegionDataset(VAL_DIR, transform=train_ds.transform)\ntest_ds  = WasteRegionDataset(TEST_DIR, transform=val_ds.transform)\n\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\ntrain_sampler = WeightedRandomSampler(train_ds.sample_weights, len(train_ds.sample_weights), replacement=True)\ntrain_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler)\nval_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)\ntest_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Train multiple configurations of model and save each model uniquely in MODELS_DIR/LOGS_DIR\n# This version supports resuming: if a checkpoint for a config exists, it skips to the next config!\n# If interrupted, just rerun -- it will not repeat finished jobs.\n# Added: Modular checkpoint/resume function, partial checkpoint detection/resume, and periodic partial saves.\n\nimport os\nimport csv\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom timm import create_model\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.cuda.amp import GradScaler\nimport pickle\n\nfrom step0_setup_folders_and_seed import set_seed\nfrom step6_dataset_and_loader import train_loader, val_loader, train_ds\n\nMODELS_DIR = '/kaggle/working/outputs/models'\nLOGS_DIR = '/kaggle/working/outputs/logs'\nnum_classes = len(train_ds.label2id)\n\ndef get_resume_checkpoint(run_id, model_dir):\n    \"\"\"\n    Check for full or partial checkpoints for a run_id.\n    Returns (resume_path, is_partial) or (None, False) if nothing found.\n    \"\"\"\n    full_ckpt = os.path.join(model_dir, f\"{run_id}_best.pth\")\n    part_ckpt = os.path.join(model_dir, f\"{run_id}_partial.pth\")\n    if os.path.isfile(full_ckpt):\n        print(f\"✅ Skipping {run_id}: Full checkpoint exists.\")\n        return (full_ckpt, False)\n    elif os.path.isfile(part_ckpt):\n        print(f\"⏸️ Resuming {run_id}: Partial checkpoint found.\")\n        return (part_ckpt, True)\n    else:\n        return (None, False)\n\ndef train_one_model(model_name, use_mixup=True, label_smooth=0.1, seed=42, patience=15, save_as=None, resume_ckpt=None):\n    set_seed(seed)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = create_model(model_name, pretrained=True, num_classes=num_classes).to(device)\n\n    # Optional: resume from partial checkpoint if provided\n    if resume_ckpt is not None:\n        print(f\"Loading weights from {resume_ckpt}\")\n        model.load_state_dict(torch.load(resume_ckpt, map_location=device))\n\n    labels = [label for _, label in train_ds.samples]\n    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smooth)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, num_warmup_steps=3 * len(train_loader),\n        num_training_steps=len(train_loader) * 100\n    )\n    scaler = GradScaler()\n\n    def mixup_data(x, y, alpha=0.4):\n        lam = np.random.beta(alpha, alpha)\n        index = torch.randperm(x.size(0)).to(x.device)\n        mixed_x = lam * x + (1 - lam) * x[index]\n        y_a, y_b = y, y[index]\n        return mixed_x, y_a, y_b, lam\n\n    def mixup_criterion(criterion, pred, y_a, y_b, lam):\n        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'train_acc': []}\n    best_val_loss = float('inf')\n    no_improve_epochs = 0\n    save_path = os.path.join(MODELS_DIR, f\"{save_as}_best.pth\" if save_as else f\"{model_name}_best.pth\")\n    partial_path = os.path.join(MODELS_DIR, f\"{save_as}_partial.pth\")\n\n    # If resuming from partial, try to restore optimizer/scheduler states (advanced -- not required for most use)\n    # For simplicity, only model weights are restored here.\n\n    # Optionally, you can load history up to now here for even better resume (not implemented for brevity)\n\n    for epoch in range(5):  # Change to your preferred number of epochs\n        model.train()\n        total_loss = 0\n        train_correct = train_total = 0\n\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            if use_mixup:\n                imgs, y_a, y_b, lam = mixup_data(imgs, labels)\n            else:\n                y_a, y_b, lam = labels, labels, 1.0\n\n            with torch.cuda.amp.autocast():\n                outputs = model(imgs)\n                loss = mixup_criterion(criterion, outputs, y_a, y_b, lam) if use_mixup else criterion(outputs, labels)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n\n            # Training accuracy (only valid if mixup disabled)\n            if not use_mixup:\n                preds = outputs.argmax(dim=1)\n                train_correct += (preds == labels).sum().item()\n                train_total += labels.size(0)\n\n        scheduler.step()\n        avg_train_loss = total_loss / len(train_loader)\n        history['train_loss'].append(avg_train_loss)\n        train_acc = train_correct / train_total if (not use_mixup and train_total > 0) else None\n        history['train_acc'].append(train_acc)\n\n        model.eval()\n        val_loss = 0\n        correct = total = 0\n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                imgs, labels = imgs.to(device), labels.to(device)\n                outputs = model(imgs)\n                val_loss += criterion(outputs, labels).item()\n                preds = outputs.argmax(dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n\n        avg_val_loss = val_loss / len(val_loader)\n        val_acc = correct / total\n        history['val_loss'].append(avg_val_loss)\n        history['val_acc'].append(val_acc)\n\n        print(f\"[{model_name}] Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | \"\n              f\"Val Loss: {avg_val_loss:.4f} | \"\n              f\"Val Acc: {val_acc:.4f}\" +\n              (f\" | Train Acc: {train_acc:.4f}\" if train_acc is not None else \"\"))\n\n        # Save partial checkpoint after every epoch (for resume if interrupted)\n        torch.save(model.state_dict(), partial_path)\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            no_improve_epochs = 0\n            torch.save(model.state_dict(), save_path)\n            print(\"🔸 New best model saved.\")\n        else:\n            no_improve_epochs += 1\n            if no_improve_epochs >= patience:\n                print(\"🛑 Early stopping triggered.\")\n                break\n\n    # Save training history\n    pd.DataFrame(history).to_csv(os.path.join(LOGS_DIR, f\"{save_as}_history.csv\"), index=False)\n\n    # If finished successfully, remove partial checkpoint file\n    try:\n        if os.path.exists(partial_path):\n            os.remove(partial_path)\n    except Exception as e:\n        print(f\"Warning: could not remove partial checkpoint: {e}\")\n\n    return model, history\n\n# --- ablation runs with modular checkpoint skip/resume and partial detection ---\nmodel_list = ['efficientnet_b0']  # You can add more models\nseeds = [42, 123, 777]\nmixup_options = [True, False]\nlabel_smoothings = [0.1, 0.0]\n\nall_histories = {}\nresults_csv_path = os.path.join(LOGS_DIR, \"ablation_results.csv\")\nall_histories_path = os.path.join(LOGS_DIR, \"all_histories.pkl\")\nresume = os.path.exists(results_csv_path)\n\n# If resuming, load all_histories so far (if exists)\nif os.path.exists(all_histories_path):\n    with open(all_histories_path, \"rb\") as f:\n        all_histories = pickle.load(f)\n\n# CSV exists? Open in append mode. Else, write header.\ncsv_mode = \"a\" if resume else \"w\"\nwith open(results_csv_path, csv_mode, newline='') as f:\n    writer = csv.writer(f)\n    if not resume:\n        writer.writerow([\"Model\", \"Seed\", \"Mixup\", \"Label Smoothing\", \"Val Accuracy\", \"Checkpoint\"])\n    for model_name in model_list:\n        for mixup in mixup_options:\n            for smooth in label_smoothings:\n                for seed in seeds:\n                    run_id = f\"{model_name}_mixup{mixup}_smooth{smooth}_seed{seed}\"\n                    checkpoint_path = os.path.join(MODELS_DIR, f\"{run_id}_best.pth\")\n                    history_path = os.path.join(LOGS_DIR, f\"{run_id}_history.csv\")\n\n                    # Use modular function for checkpoint skip/resume logic!\n                    resume_ckpt, is_partial = get_resume_checkpoint(run_id, MODELS_DIR)\n                    if resume_ckpt and not is_partial:\n                        # Full checkpoint exists, skip this run\n                        if os.path.isfile(history_path):\n                            all_histories[run_id] = pd.read_csv(history_path).to_dict('list')\n                        continue\n\n                    print(f\"🔁 Training: {run_id}\")\n                    model, history = train_one_model(\n                        model_name=model_name,\n                        use_mixup=mixup,\n                        label_smooth=smooth,\n                        seed=seed,\n                        patience=15,\n                        save_as=run_id,\n                        resume_ckpt=resume_ckpt if is_partial else None\n                    )\n                    final_val_acc = history['val_acc'][-1]\n                    writer.writerow([model_name, seed, mixup, smooth, final_val_acc, checkpoint_path])\n                    all_histories[run_id] = history\n                    # Immediately save histories for resume safety\n                    with open(all_histories_path, \"wb\") as hf:\n                        pickle.dump(all_histories, hf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Summarize ablation results and print/save summary\n\nimport pandas as pd\n\ndf = pd.read_csv(results_csv_path)\nprint(\"\\n📝 All ablation results:\\n\")\nprint(df)\n\nsummary = df.groupby([\"Model\", \"Mixup\", \"Label Smoothing\"])[\"Val Accuracy\"].agg(['mean', 'std']).reset_index()\nprint(\"\\n📊 Ablation Results (mean ± std across seeds):\\n\")\nprint(summary)\n\nbest_row = df.loc[df[\"Val Accuracy\"].idxmax()]\nprint(\"\\n\\n🏆 Best configuration:\\n\\n\", best_row)\n\n# Save summary to metrics\nsummary_path = os.path.join(METRICS_DIR, \"ablation_summary.csv\")\nsummary.to_csv(summary_path, index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n  #Step 11: Statistical Significance Testing\n- Prints a clean table with short config labels for easy reading.\n- Includes judgment about whether each difference is statistically significant (p < 0.05).\n- Includes a summary at the end to help beginners interpret the results.\n\"\"\"\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom scipy.stats import ttest_rel, wilcoxon\n\nOUTPUT_ROOT = '/kaggle/working/outputs'\nLOGS_DIR = os.path.join(OUTPUT_ROOT, \"logs\")\nSTATS_DIR = os.path.join(OUTPUT_ROOT, \"stats\")\nresults_csv_path = os.path.join(LOGS_DIR, \"ablation_results.csv\")\nstats_csv_path = os.path.join(STATS_DIR, \"statistical_tests.csv\")\nstats_md_path = os.path.join(STATS_DIR, \"statistical_tests.md\")\n\ndef interpret_p(p):\n    \"\"\"Return interpretation string for a p-value.\"\"\"\n    if p < 0.01:\n        return \"Highly significant (p < 0.01)\"\n    elif p < 0.05:\n        return \"Significant (p < 0.05)\"\n    elif p < 0.10:\n        return \"Suggestive (p < 0.10)\"\n    else:\n        return \"Not significant (p ≥ 0.10)\"\n\nif os.path.exists(results_csv_path):\n    df = pd.read_csv(results_csv_path)\n    group_cols = [\"Model\", \"Mixup\", \"Label Smoothing\"]\n    records = []\n\n    # Get all unique ablation config pairs (ignoring seed)\n    configs = df[group_cols].drop_duplicates().to_dict('records')\n    pairs = list(itertools.combinations(configs, 2))\n\n    def config_short(cfg):\n        return f\"{cfg['Model']}, Mixup={cfg['Mixup']}, LS={cfg['Label Smoothing']}\"\n\n    for cfg_a, cfg_b in pairs:\n        mask_a = (df[group_cols] == pd.Series(cfg_a)).all(axis=1)\n        mask_b = (df[group_cols] == pd.Series(cfg_b)).all(axis=1)\n        vals_a = df.loc[mask_a, \"Val Accuracy\"].sort_index().values\n        vals_b = df.loc[mask_b, \"Val Accuracy\"].sort_index().values\n\n        # Only if both configs have same number of seeds/runs\n        if len(vals_a) == len(vals_b) and len(vals_a) > 1:\n            t_stat, t_p = ttest_rel(vals_a, vals_b)\n            try:\n                w_stat, w_p = wilcoxon(vals_a, vals_b)\n            except Exception:\n                w_stat, w_p = None, None\n\n            # Add judgment columns\n            t_judgement = interpret_p(t_p)\n            w_judgement = interpret_p(w_p if w_p is not None else 1.0)\n\n            records.append({\n                \"ConfigA\": config_short(cfg_a),\n                \"ConfigB\": config_short(cfg_b),\n                \"meanA\": np.mean(vals_a),\n                \"meanB\": np.mean(vals_b),\n                \"t_stat\": t_stat,\n                \"t_p\": t_p,\n                \"t_judgement\": t_judgement,\n                \"wilcoxon_stat\": w_stat,\n                \"wilcoxon_p\": w_p,\n                \"wilcoxon_judgement\": w_judgement,\n            })\n\n    # Save results as CSV\n    pd.DataFrame(records).to_csv(stats_csv_path, index=False)\n\n    # Save results as Markdown\n    with open(stats_md_path, \"w\") as f:\n        f.write(\"| ConfigA | ConfigB | meanA | meanB | t_stat | t_p | t_judgement | wilcoxon_stat | wilcoxon_p | wilcoxon_judgement |\\n\")\n        f.write(\"|---------|---------|-------|-------|--------|-----|-------------|---------------|------------|--------------------|\\n\")\n        for r in records:\n            f.write(f\"| {r['ConfigA']} | {r['ConfigB']} | {r['meanA']:.4f} | {r['meanB']:.4f} | {r['t_stat']:.3f} | {r['t_p']:.4f} | {r['t_judgement']} | {r['wilcoxon_stat']} | {r['wilcoxon_p']:.4f} | {r['wilcoxon_judgement']} |\\n\")\n\n    print(f\"🧪 Statistical significance test results saved to:\\n  {stats_csv_path}\\n  {stats_md_path}\")\n\n    # Print nicely formatted table with judgment\n    print(\"\\nStatistical significance test results:\\n\")\n    print(\"| ConfigA | ConfigB | meanA | meanB | t_stat | t_p | t_judgement | wilcoxon_stat | wilcoxon_p | wilcoxon_judgement |\")\n    print(\"|---------|---------|-------|-------|--------|-------|-------------|---------------|------------|--------------------|\")\n    for r in records:\n        print(f\"| {r['ConfigA']} | {r['ConfigB']} | {r['meanA']:.4f} | {r['meanB']:.4f} | {r['t_stat']:.3f} | {r['t_p']:.4f} | {r['t_judgement']} | {r['wilcoxon_stat']} | {r['wilcoxon_p']:.4f} | {r['wilcoxon_judgement']} |\")\n\n    # Beginner-friendly summary\n    print(\"\\nHow to interpret these results:\")\n    print(\"- 't_p' and 'wilcoxon_p' are p-values. If they are less than 0.05, the difference between the two configurations is considered statistically significant (unlikely due to chance).\")\n    print(\"- The 't_judgement' and 'wilcoxon_judgement' columns say if the difference is significant.\")\n    print(\"- 'meanA' and 'meanB' show the average validation accuracy for each configuration. Higher is better.\")\n    print(\"- If all p-values are 'Not significant', then the changes you made in configuration likely did not have a meaningful effect on performance.\")\n    print(\"- If you see 'Significant' or 'Highly significant', the difference is likely real and not random. Prefer the config with higher mean accuracy.\")\n\nelse:\n    print(f\"ERROR: ablation_results.csv not found at {results_csv_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 12: Evaluate Best-Performing Model on Test Set and save metrics/plots in BEST_MODEL_DIR\n# Also plot and save the loss and accuracy curves for the best model.\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    classification_report, confusion_matrix, roc_auc_score\n)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nimport os\n\n# Load best model\nbest_checkpoint_path = best_row[\"Checkpoint\"]\nbest_model_name = best_row[\"Model\"]\n\nmodel = create_model(best_model_name, pretrained=False, num_classes=num_classes).to(device)\nmodel.load_state_dict(torch.load(best_checkpoint_path))\n\ndef evaluate_model_detailed(model, dataloader, device, label2id, show_roc_auc=True, save_metrics_dir=None, save_vis_dir=None):\n    model.eval()\n    all_preds, all_probs, all_labels = [], [], []\n    with torch.no_grad():\n        for imgs, labels in dataloader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            probs = torch.softmax(outputs, dim=1)\n            preds = outputs.argmax(dim=1)\n            all_probs.extend(probs.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n    all_probs = np.array(all_probs)\n    id2label = {v: k for k, v in label2id.items()}\n    target_names = [id2label[i] for i in sorted(id2label)]\n    print(\"\\n🔍 Classification Report (on Test Set):\\n\")\n    report_str = classification_report(all_labels, all_preds, target_names=target_names, digits=4)\n    print(report_str)\n    acc = accuracy_score(all_labels, all_preds)\n    prec = precision_score(all_labels, all_preds, average='weighted')\n    rec = recall_score(all_labels, all_preds, average='weighted')\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    print(f\"Test Accuracy:  {acc:.4f}\")\n    print(f\"Test Precision: {prec:.4f}\")\n    print(f\"Test Recall:    {rec:.4f}\")\n    print(f\"Test F1-Score:  {f1:.4f}\")\n\n    # Save metrics to BEST_MODEL_DIR as JSON\n    if save_metrics_dir:\n        metrics = {\n            \"accuracy\": acc,\n            \"precision\": prec,\n            \"recall\": rec,\n            \"f1\": f1,\n            \"classification_report\": classification_report(all_labels, all_preds, target_names=target_names, digits=4, output_dict=True)\n        }\n        with open(os.path.join(save_metrics_dir, \"test_metrics.json\"), \"w\") as f:\n            json.dump(metrics, f, indent=2)\n\n    # Confusion Matrix – Raw\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=target_names, yticklabels=target_names)\n    plt.title(\"Confusion Matrix – Raw (Test Set)\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    if save_vis_dir:\n        plt.savefig(os.path.join(save_vis_dir, \"confusion_matrix_raw.png\"))\n    plt.show()\n\n    # Confusion Matrix – Normalized\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Oranges',\n                xticklabels=target_names, yticklabels=target_names)\n    plt.title(\"Confusion Matrix – Normalized (Test Set)\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    if save_vis_dir:\n        plt.savefig(os.path.join(save_vis_dir, \"confusion_matrix_normalized.png\"))\n    plt.show()\n\n    # ROC AUC Score\n    if show_roc_auc:\n        try:\n            from sklearn.preprocessing import label_binarize\n            y_true_bin = label_binarize(all_labels, classes=list(range(len(label2id))))\n            auc = roc_auc_score(y_true_bin, all_probs, multi_class='ovr')\n            print(f\"Test ROC AUC (Multiclass OVR): {auc:.4f}\")\n            if save_metrics_dir:\n                with open(os.path.join(save_metrics_dir, \"test_roc_auc.txt\"), \"w\") as f:\n                    f.write(f\"{auc:.6f}\\n\")\n        except Exception as e:\n            print(f\"⚠️ ROC-AUC computation failed: {e}\")\n\n# --- Evaluate on test set and save metrics/plots ---\nevaluate_model_detailed(\n    model, test_loader, device, train_ds.label2id,\n    save_metrics_dir=BEST_MODEL_DIR,\n    save_vis_dir=BEST_MODEL_DIR\n)\n\n# --- Plot and save the loss and accuracy curves of the best model ---\n\n# Try to get the best run id. This should match the key in all_histories for the best model.\nif \"run_id\" in best_row:\n    best_run_id = best_row[\"run_id\"]\nelse:\n    best_run_id = os.path.basename(best_checkpoint_path).replace(\"_best.pth\", \"\")\n\nif best_run_id in all_histories:\n    history = all_histories[best_run_id]\n    epochs = range(1, len(history['train_loss']) + 1)\n    plt.figure(figsize=(16, 5))\n    # Loss Curve\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history['train_loss'], label='Train Loss', marker='o')\n    plt.plot(epochs, history['val_loss'], label='Val Loss', marker='x')\n    plt.title(\"Loss Curve\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    # Accuracy Curve\n    plt.subplot(1, 2, 2)\n    if 'train_acc' in history and any(x is not None for x in history['train_acc']):\n        train_acc_plot = [x if x is not None else np.nan for x in history['train_acc']]\n        plt.plot(epochs, train_acc_plot, label='Train Accuracy', marker='d', color='blue')\n    plt.plot(epochs, history['val_acc'], label='Val Accuracy', marker='s', color='green')\n    plt.title(\"Accuracy Curve\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n    plt.suptitle(f\"Best Model Training Progress – {best_run_id}\", fontsize=14)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    # Save in BEST_MODEL_DIR\n    plt.savefig(os.path.join(BEST_MODEL_DIR, f\"curve_{best_run_id}.png\"))\n    plt.show()\nelse:\n    print(f\"Best run's training history not found for plotting (run_id: {best_run_id})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 13: Visualize side-by-side: original, Grad-CAM++, Score-CAM for best model predictions\n\nprint(f\"Loaded model for interpretability: {best_checkpoint_path}\")\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nfrom pytorch_grad_cam import GradCAMPlusPlus, ScoreCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# --- Configuration ---\nN_SAMPLES = 10  # Number of samples to visualize\ninterp_save_dir = INTERP_DIR\nos.makedirs(interp_save_dir, exist_ok=True)\n\n# --- Load best model ---\n# (Assume model, best_checkpoint_path, test_loader, and train_ds.label2id are already loaded)\nmodel.eval()\n\n# Helper: get sample images and predictions from test set\ndef get_sample_images_and_preds(model, dataloader, device, n=10):\n    samples = []\n    with torch.no_grad():\n        for imgs, labels in dataloader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            preds = outputs.argmax(dim=1)\n            for i in range(imgs.size(0)):\n                samples.append((imgs[i].cpu(), labels[i].cpu().item(), preds[i].cpu().item()))\n                if len(samples) >= n:\n                    return samples\n    return samples\n\nsamples = get_sample_images_and_preds(model, test_loader, device, n=N_SAMPLES)\n\n# Prepare CAM methods\n# Choose the final conv layer as target layer for CAMs (adjust as needed)\ntry:\n    target_layer = model.conv_head\nexcept AttributeError:\n    # Fallback: get the last Conv2d layer in the model\n    import torch.nn as nn\n    target_layer = None\n    for m in reversed(list(model.modules())):\n        if isinstance(m, nn.Conv2d):\n            target_layer = m\n            break\n    if target_layer is None:\n        raise RuntimeError(\"Could not find a Conv2d layer for CAM visualization.\")\n\ngradcampp = GradCAMPlusPlus(model=model, target_layers=[target_layer])\nscorecam = ScoreCAM(model=model, target_layers=[target_layer])\n\n# Visualization loop\nfor idx, (img_tensor, label, pred) in enumerate(samples):\n    # Prepare input and image\n    input_tensor = img_tensor.unsqueeze(0)\n    # Undo normalization for display\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    img_np = img_tensor.permute(1,2,0).numpy() * std + mean\n    img_np = np.clip(img_np, 0, 1)\n    \n    # Grad-CAM++\n    grayscale_cam_pp = gradcampp(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred)])[0]\n    campp_img = show_cam_on_image(img_np, grayscale_cam_pp, use_rgb=True)\n    \n    # Score-CAM\n    grayscale_cam_score = scorecam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred)])[0]\n    camscore_img = show_cam_on_image(img_np, grayscale_cam_score, use_rgb=True)\n    \n    # Plot and save\n    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n    axs[0].imshow(img_np)\n    axs[0].set_title(\"Original\")\n    axs[0].axis(\"off\")\n    axs[1].imshow(campp_img)\n    axs[1].set_title(\"Grad-CAM++\")\n    axs[1].axis(\"off\")\n    axs[2].imshow(camscore_img)\n    axs[2].set_title(\"Score-CAM\")\n    axs[2].axis(\"off\")\n    gt_class = [k for k, v in train_ds.label2id.items() if v == label][0]\n    pred_class = [k for k, v in train_ds.label2id.items() if v == pred][0]\n    fig.suptitle(f\"GT: {gt_class} | Pred: {pred_class}\", fontsize=14)\n    plt.tight_layout()\n    save_path = os.path.join(interp_save_dir, f\"interp_{idx+1}_gt_{gt_class}_pred_{pred_class}.png\")\n    plt.savefig(save_path)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}