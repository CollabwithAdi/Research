{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Create output folders for all stages of the adaptive ensemble pipeline\n\nimport os\n\nKAGGLE_WORKING = '/kaggle/working'\nOUTPUT_ROOT = os.path.join(KAGGLE_WORKING, \"outputs\")\nFEATURES_DIR = os.path.join(OUTPUT_ROOT, \"features\")           # for extracted regions/crops\nSPLITS_DIR = os.path.join(OUTPUT_ROOT, \"splits\")               # for train/val/test splits\nMODELS_DIR = os.path.join(OUTPUT_ROOT, \"models\")               # trained models\nLOGS_DIR = os.path.join(OUTPUT_ROOT, \"logs\")                   # training logs, CSVs\nMETRICS_DIR = os.path.join(OUTPUT_ROOT, \"metrics\")             # evaluation metrics as JSON/CSV\nVIS_DIR = os.path.join(OUTPUT_ROOT, \"visualizations\")          # plots, confusion matrix, etc.\nENSEMBLE_DIR = os.path.join(OUTPUT_ROOT, \"ensemble\")           # specific directory for ensemble results\nINTERP_DIR = os.path.join(OUTPUT_ROOT, \"interpretability\")     # interpretability visualizations\nEXTRACT_VIS_DIR = os.path.join(VIS_DIR, \"extracted_regions\")   # visualizations of extracted regions\n\nfor d in [\n    OUTPUT_ROOT, FEATURES_DIR, SPLITS_DIR, MODELS_DIR, LOGS_DIR,\n    METRICS_DIR, VIS_DIR, ENSEMBLE_DIR, INTERP_DIR, EXTRACT_VIS_DIR\n]:\n    os.makedirs(d, exist_ok=True)\n\n# Create test directory (common across all folds)\nTEST_DIR = os.path.join(SPLITS_DIR, \"test\")\nos.makedirs(TEST_DIR, exist_ok=True)\n\nprint(f\"âœ… Directory structure created for journal-quality outputs\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Set Global Seed for Reproducibility\n\nimport random\nimport numpy as np\nimport torch\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Install essential libraries for the adaptive ensemble\n\n!pip install torch torchvision\n!pip install timm transformers roboflow\n!pip install grad-cam matplotlib seaborn pandas numpy tqdm scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Download the COCO-format segmentation dataset from Roboflow\n\nfrom roboflow import Roboflow\nimport os\nimport json\n\nrf = Roboflow(api_key=\"Mvt9FCxE4mY6vBy5OG08\")  # Replace with your key if needed\nproject = rf.workspace(\"urban-lake-wastef\").project(\"another_approach_try\")\nversion = project.version(4)\ndataset = version.download(\"coco-segmentation\")\n\nDATA_ROOT = dataset.location\nTRAIN_JSON = os.path.join(DATA_ROOT, 'train', '_annotations.coco.json')\nIMG_DIR = os.path.join(DATA_ROOT, 'train')\n\nprint(f\"Dataset downloaded to: {DATA_ROOT}\")\nprint(f\"Train JSON: {TRAIN_JSON}\")\nprint(f\"Image directory: {IMG_DIR}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Extract foreground objects from COCO masks and save cropped images in FEATURES_DIR\n\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\nwith open(TRAIN_JSON) as f:\n    ann_data = json.load(f)\n\ncat_map = {c['id']: c['name'] for c in ann_data['categories']}\nprint(f\"Waste categories: {list(cat_map.values())}\")\n\n# Track statistics\nextracted_count = {cat: 0 for cat in cat_map.values()}\nextraction_failures = 0\n\nfor ann in ann_data['annotations']:\n    try:\n        img_info = next(img for img in ann_data['images'] if img['id'] == ann['image_id'])\n        img_path = os.path.join(IMG_DIR, img_info['file_name'])\n        img = Image.open(img_path).convert('RGB')\n\n        seg = ann['segmentation']\n        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n\n        for poly in seg:\n            pts = np.array(poly).reshape(-1, 2)\n            m = Image.new('L', (img_info['width'], img_info['height']), 0)\n            ImageDraw.Draw(m).polygon([tuple(p) for p in pts], outline=1, fill=1)\n            mask = np.maximum(mask, np.array(m))\n\n        if mask.sum() < 100:  # Skip very small masks\n            extraction_failures += 1\n            continue\n\n        region = np.array(img) * mask[:, :, None]\n        region_img = Image.fromarray(region)\n\n        label = cat_map[ann['category_id']]\n        out_dir = os.path.join(FEATURES_DIR, label)\n        os.makedirs(out_dir, exist_ok=True)\n        base = os.path.splitext(img_info['file_name'])[0]\n        out_path = os.path.join(out_dir, f\"{base}_{ann['id']}.png\")\n        region_img.save(out_path)\n        \n        extracted_count[label] += 1\n    except Exception as e:\n        print(f\"Error processing annotation {ann['id']}: {e}\")\n        extraction_failures += 1\n\nprint(f\"âœ… Extracted regions saved to: {FEATURES_DIR}\")\nprint(f\"Extraction statistics:\")\nfor category, count in extracted_count.items():\n    print(f\"  - {category}: {count} images\")\nprint(f\"  - Failed extractions: {extraction_failures}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Visualize 10 original vs masked crops for journal-quality reporting\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport random\n\ndef show_before_after_masked(original_dir, masked_dir, num_samples=10, vis_dir=None):\n    \"\"\"Visualize original images and their extracted foreground regions side-by-side\"\"\"\n    print(f\"\\nðŸ” Showing {num_samples} samples: original vs extracted region\")\n    \n    classes = sorted([d for d in os.listdir(masked_dir) if os.path.isdir(os.path.join(masked_dir, d))])\n    selected_images = []\n    \n    while len(selected_images) < num_samples and classes:\n        # Select a class randomly (weighted by available samples)\n        class_sample_counts = {}\n        for c in classes:\n            class_dir = os.path.join(masked_dir, c)\n            if os.path.isdir(class_dir):\n                files = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.png'))]\n                if files:\n                    class_sample_counts[c] = len(files)\n        \n        if not class_sample_counts:\n            break\n            \n        # Weight classes by inverse of sample count to balance visualization\n        total_samples = sum(class_sample_counts.values())\n        class_weights = {c: 1.0/count for c, count in class_sample_counts.items()}\n        weight_sum = sum(class_weights.values())\n        class_weights = {c: w/weight_sum for c, w in class_weights.items()}\n        \n        chosen_class = random.choices(\n            list(class_weights.keys()),\n            weights=list(class_weights.values()),\n            k=1\n        )[0]\n        \n        class_mask_dir = os.path.join(masked_dir, chosen_class)\n        mask_files = os.listdir(class_mask_dir)\n        if not mask_files:\n            continue\n            \n        chosen_file = random.choice(mask_files)\n        \n        # Avoid duplicates\n        if (chosen_class, chosen_file) not in selected_images:\n            selected_images.append((chosen_class, chosen_file))\n    \n    # Create figure for all samples\n    fig, axs = plt.subplots(num_samples, 2, figsize=(12, 4*num_samples))\n    \n    for idx, (class_name, file_name) in enumerate(selected_images):\n        masked_path = os.path.join(masked_dir, class_name, file_name)\n        \n        # Extract original image name from the masked filename\n        # Format: original_name_annotation_id.png\n        original_basename = \"_\".join(file_name.split(\"_\")[:-1])  # Remove last part after underscore\n        \n        # Try different extensions\n        possible_exts = [\".jpg\", \".png\", \".jpeg\"]\n        original_path = None\n        for ext in possible_exts:\n            candidate = os.path.join(original_dir, original_basename + ext)\n            if os.path.exists(candidate):\n                original_path = candidate\n                break\n        \n        if not original_path:\n            print(f\"âš ï¸ Original not found for: {original_basename}\")\n            # Use a placeholder instead\n            axs[idx, 0].text(0.5, 0.5, \"Original image not found\", \n                             ha='center', va='center')\n            axs[idx, 0].set_title(\"Original (Not Found)\")\n            axs[idx, 0].axis(\"off\")\n        else:\n            original_img = Image.open(original_path).convert(\"RGB\")\n            axs[idx, 0].imshow(original_img)\n            axs[idx, 0].set_title(\"Original\")\n            axs[idx, 0].axis(\"off\")\n        \n        # Display masked image\n        masked_img = Image.open(masked_path).convert(\"RGB\")\n        axs[idx, 1].imshow(masked_img)\n        axs[idx, 1].set_title(f\"Extracted Region ({class_name})\")\n        axs[idx, 1].axis(\"off\")\n    \n    plt.tight_layout()\n    \n    if vis_dir:\n        save_path = os.path.join(vis_dir, \"original_vs_extracted.png\")\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\"âœ… Visualization saved to: {save_path}\")\n    \n    plt.show()\n\n# Show examples of extracted regions vs original images\nshow_before_after_masked(\n    original_dir=IMG_DIR,\n    masked_dir=FEATURES_DIR,\n    num_samples=10,\n    vis_dir=EXTRACT_VIS_DIR\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Configure K-Fold Cross-Validation\n\nfrom sklearn.model_selection import KFold, train_test_split\n\n# K-fold configuration\nK_FOLDS = 5\nEPOCHS_BASE_MODELS = 200  # Increased from 30\nEPOCHS_ENSEMBLE = 50      # Increased from 15\nTEST_SPLIT = 0.2          # 20% of data for final test set\n\nprint(f\"âœ… K-Fold Cross-Validation configured with {K_FOLDS} folds\")\nprint(f\"âœ… Epochs for base models: {EPOCHS_BASE_MODELS}\")\nprint(f\"âœ… Epochs for ensemble model: {EPOCHS_ENSEMBLE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Implement K-Fold Cross-Validation Split\n\nimport shutil\nimport numpy as np\nfrom sklearn.model_selection import KFold, train_test_split\n\n# Dictionary to track class samples for each split\nsplit_counts = {'train': {}, 'val': {}, 'test': {}}\nfold_counts = []\n\n# Create split directories\nfor fold in range(K_FOLDS):\n    os.makedirs(os.path.join(SPLITS_DIR, f\"fold_{fold}\", \"train\"), exist_ok=True)\n    os.makedirs(os.path.join(SPLITS_DIR, f\"fold_{fold}\", \"val\"), exist_ok=True)\n\n# Process each class\nfor class_name in os.listdir(FEATURES_DIR):\n    class_path = os.path.join(FEATURES_DIR, class_name)\n    if not os.path.isdir(class_path):\n        continue\n    \n    files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n    \n    if len(files) < K_FOLDS + 1:  # Need at least one sample per fold plus test\n        print(f\"âš ï¸ Warning: Class '{class_name}' has only {len(files)} samples. Skipping.\")\n        continue\n    \n    # First split out the test set\n    train_val_files, test_files = train_test_split(files, test_size=TEST_SPLIT, random_state=42)\n    \n    # Create test directory for this class\n    test_class_dir = os.path.join(SPLITS_DIR, \"test\", class_name)\n    os.makedirs(test_class_dir, exist_ok=True)\n    \n    # Copy test files\n    for f in test_files:\n        shutil.copy2(os.path.join(class_path, f), os.path.join(test_class_dir, f))\n    \n    # Track test counts\n    split_counts['test'][class_name] = len(test_files)\n    \n    # Implement K-fold for the remaining data\n    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n    \n    # Convert to numpy array for indexing\n    train_val_files = np.array(train_val_files)\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_val_files)):\n        train_files = train_val_files[train_idx]\n        val_files = train_val_files[val_idx]\n        \n        # Create directories for this fold\n        fold_train_class_dir = os.path.join(SPLITS_DIR, f\"fold_{fold}\", \"train\", class_name)\n        fold_val_class_dir = os.path.join(SPLITS_DIR, f\"fold_{fold}\", \"val\", class_name)\n        \n        os.makedirs(fold_train_class_dir, exist_ok=True)\n        os.makedirs(fold_val_class_dir, exist_ok=True)\n        \n        # Copy train files\n        for f in train_files:\n            shutil.copy2(os.path.join(class_path, f), os.path.join(fold_train_class_dir, f))\n            \n        # Copy val files\n        for f in val_files:\n            shutil.copy2(os.path.join(class_path, f), os.path.join(fold_val_class_dir, f))\n        \n        # Track counts for this fold\n        if fold == 0:  # Initialize dictionary for first fold\n            split_counts['train'][class_name] = len(train_files)\n            split_counts['val'][class_name] = len(val_files)\n        else:\n            # For other folds, we'll store separately\n            if fold >= len(fold_counts):\n                fold_counts.append({'train': {}, 'val': {}})\n            \n            fold_counts[fold-1]['train'][class_name] = len(train_files)\n            fold_counts[fold-1]['val'][class_name] = len(val_files)\n\n# Visualize class distribution across splits (for first fold and test)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create DataFrame for visualization\nsplit_df = pd.DataFrame({\n    'Train (Fold 0)': pd.Series(split_counts['train']),\n    'Validation (Fold 0)': pd.Series(split_counts['val']),\n    'Test': pd.Series(split_counts['test'])\n}).fillna(0).astype(int)\n\n# Plot\nplt.figure(figsize=(12, 6))\nsplit_df.plot(kind='bar', figsize=(12, 6))\nplt.title('Class Distribution Across Splits (Fold 0 shown)')\nplt.xlabel('Class')\nplt.ylabel('Number of Images')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Save visualization\nplt.savefig(os.path.join(VIS_DIR, 'class_distribution_fold0.png'), dpi=300, bbox_inches='tight')\nplt.show()\n\n# Save split statistics\nsplit_df.to_csv(os.path.join(METRICS_DIR, 'split_statistics.csv'))\n\n# Calculate total number of samples\ntrain_total = sum(sum(cls.values()) for cls in [split_counts['train']] + [fc['train'] for fc in fold_counts])\nval_total = sum(sum(cls.values()) for cls in [split_counts['val']] + [fc['val'] for fc in fold_counts])\ntest_total = sum(split_counts['test'].values())\n\nprint(f\"âœ… K-Fold Cross-Validation split completed with {K_FOLDS} folds\")\nprint(f\"Training samples: ~{train_total // K_FOLDS} per fold\")\nprint(f\"Validation samples: ~{val_total // K_FOLDS} per fold\")\nprint(f\"Test samples: {test_total}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Define Dataset and Data Loaders with K-fold support\n\nfrom torch.utils.data import Dataset, WeightedRandomSampler, DataLoader\nfrom torchvision import transforms\nfrom collections import defaultdict\nfrom PIL import Image\n\nclass WasteRegionDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        \"\"\"Dataset for waste regions with class balancing via sample weights\"\"\"\n        self.samples = []\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.03),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        self.label2id = {}\n        class_counts = defaultdict(int)\n        classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        \n        for c in classes:\n            class_dir = os.path.join(root_dir, c)\n            if not os.path.isdir(class_dir):\n                continue\n            for f in os.listdir(class_dir):\n                if f.lower().endswith(('.jpg', '.png', '.jpeg')):\n                    self.samples.append((os.path.join(class_dir, f), self.label2id[c]))\n                    class_counts[self.label2id[c]] += 1\n        \n        # Create sample weights for balanced sampling\n        if class_counts:  # Only if we have some samples\n            self.sample_weights = [1.0 / class_counts[label] for _, label in self.samples]\n        else:\n            self.sample_weights = []\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        img = Image.open(path).convert('RGB')\n        img = self.transform(img)\n        return img, label\n\n# Create test transform with only deterministic operations\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Function to create dataloaders for a specific fold\ndef create_fold_dataloaders(fold_idx, batch_size=16):\n    \"\"\"Create train and validation dataloaders for the specified fold\"\"\"\n    # Paths for this fold\n    fold_train_dir = os.path.join(SPLITS_DIR, f\"fold_{fold_idx}\", \"train\")\n    fold_val_dir = os.path.join(SPLITS_DIR, f\"fold_{fold_idx}\", \"val\")\n    \n    # Create datasets\n    fold_train_ds = WasteRegionDataset(fold_train_dir)\n    fold_val_ds = WasteRegionDataset(fold_val_dir, transform=test_transform)\n    \n    # Create dataloaders with weighted sampling for training\n    if fold_train_ds.sample_weights:  # Check if we have samples\n        train_sampler = WeightedRandomSampler(\n            fold_train_ds.sample_weights, \n            len(fold_train_ds.sample_weights), \n            replacement=True\n        )\n        fold_train_loader = DataLoader(\n            fold_train_ds, batch_size=batch_size, \n            sampler=train_sampler, num_workers=2\n        )\n    else:\n        print(f\"âš ï¸ Warning: No training samples found for fold {fold_idx}!\")\n        fold_train_loader = DataLoader(\n            fold_train_ds, batch_size=batch_size, \n            shuffle=True, num_workers=2\n        )\n    \n    fold_val_loader = DataLoader(\n        fold_val_ds, batch_size=batch_size, \n        shuffle=False, num_workers=2\n    )\n    \n    return fold_train_loader, fold_val_loader, fold_train_ds.label2id\n\n# Create test dataloader (common for all folds)\ntest_ds = WasteRegionDataset(os.path.join(SPLITS_DIR, \"test\"), transform=test_transform)\ntest_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=2)\n\n# Get number of classes from test dataset\nnum_classes = len(test_ds.label2id)\nclass_names = [k for k, v in sorted(test_ds.label2id.items(), key=lambda x: x[1])]\n\nprint(f\"âœ… Dataset and DataLoader configuration for K-fold completed\")\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Test samples: {len(test_ds)}\")\nprint(\"\\nClass names:\")\nfor i, name in enumerate(class_names):\n    print(f\"  - Class {i}: {name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Define the Adaptive Weighted Ensemble Model\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm import create_model\n\n# Base model loader function\ndef get_base_model(model_name, num_classes):\n    \"\"\"Load a pretrained model and modify for waste classification\"\"\"\n    model = create_model(model_name, pretrained=True, num_classes=num_classes)\n    return model\n\n# Adaptive Weighted Ensemble Class\nclass AdaptiveWeightedEnsemble(nn.Module):\n    def __init__(self, models, num_classes, base_weights=None):\n        \"\"\"\n        Adaptive Weighted Ensemble model that dynamically adjusts weights based on input\n        \n        Parameters:\n        -----------\n        models : list of nn.Module\n            List of pre-trained models to ensemble\n        num_classes : int\n            Number of output classes\n        base_weights : list, optional\n            Initial weights for each model, normalized internally\n        \"\"\"\n        super().__init__()\n        self.models = nn.ModuleList(models)\n        self.n_models = len(models)\n        self.n_classes = num_classes\n        \n        # Initialize base weights\n        if base_weights is None:\n            self.base_weights = nn.Parameter(torch.ones(self.n_models) / self.n_models)\n        else:\n            weights = torch.tensor(base_weights, dtype=torch.float32)\n            weights = weights / weights.sum()  # Normalize\n            self.base_weights = nn.Parameter(weights)\n        \n        # Water condition adaptation network - learns to adapt weights based on input\n        self.condition_encoder = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=2),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(16, self.n_models),\n            nn.Softmax(dim=0)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass computing adaptive weighted ensemble prediction\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input image batch of shape [batch_size, channels, height, width]\n            \n        Returns:\n        --------\n        torch.Tensor\n            Class probabilities of shape [batch_size, n_classes]\n        \"\"\"\n        batch_size = x.size(0)\n        \n        # Get condition-specific weights\n        condition_weights = self.condition_encoder(x)  # Shape: [batch_size, n_models]\n        \n        # Get predictions from all models\n        all_outputs = []\n        for model in self.models:\n            with torch.no_grad():  # Base models are frozen\n                model_output = F.softmax(model(x), dim=1)  # [batch_size, n_classes]\n                all_outputs.append(model_output)\n        \n        # Stack all outputs: [n_models, batch_size, n_classes]\n        all_outputs = torch.stack(all_outputs)\n        \n        # Prepare weights for broadcasting - combine base weights with condition weights\n        # Reshape to [n_models, batch_size, 1]\n        weights = self.base_weights.view(-1, 1, 1) * condition_weights.transpose(0, 1).unsqueeze(-1)\n        \n        # Normalize weights across models dimension\n        weights = weights / weights.sum(dim=0, keepdim=True)\n        \n        # Apply weights and sum: [batch_size, n_classes]\n        weighted_outputs = (all_outputs * weights).sum(dim=0)\n        \n        return weighted_outputs\n\nprint(\"âœ… Adaptive Weighted Ensemble model defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 11: Define Training Functions for Individual Models and Ensemble\n\nimport torch.optim as optim\nfrom tqdm.notebook import tqdm\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import get_cosine_schedule_with_warmup\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef train_single_model(model_name, num_classes, train_loader, val_loader, device, \n                       epochs=100, patience=15, use_mixup=True, label_smoothing=0.1):\n    \"\"\"\n    Train a single model and return the trained model and its validation accuracy\n    \n    Parameters:\n    -----------\n    model_name : str\n        Name of the model architecture to train\n    num_classes : int\n        Number of output classes\n    train_loader : DataLoader\n        DataLoader for training data\n    val_loader : DataLoader\n        DataLoader for validation data\n    device : torch.device\n        Device to train on (cuda or cpu)\n    epochs : int, optional\n        Maximum number of epochs to train for\n    patience : int, optional\n        Early stopping patience (number of epochs without improvement)\n    use_mixup : bool, optional\n        Whether to use mixup data augmentation\n    label_smoothing : float, optional\n        Label smoothing factor for loss function\n        \n    Returns:\n    --------\n    tuple\n        (trained model, best validation accuracy, training history)\n    \"\"\"\n    model = get_base_model(model_name, num_classes).to(device)\n    \n    # Compute class weights for imbalanced dataset\n    labels = [label for _, label in train_loader.dataset.samples]\n    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n    \n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)\n    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-3)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, num_warmup_steps=3 * len(train_loader),\n        num_training_steps=len(train_loader) * epochs\n    )\n    \n    def mixup_data(x, y, alpha=0.4):\n        \"\"\"Applies mixup augmentation to a batch\"\"\"\n        lam = np.random.beta(alpha, alpha)\n        index = torch.randperm(x.size(0)).to(x.device)\n        mixed_x = lam * x + (1 - lam) * x[index]\n        y_a, y_b = y, y[index]\n        return mixed_x, y_a, y_b, lam\n    \n    def mixup_criterion(criterion, pred, y_a, y_b, lam):\n        \"\"\"Applies mixup to the loss calculation\"\"\"\n        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n    \n    best_val_loss = float('inf')\n    best_val_acc = 0.0\n    no_improve_epochs = 0\n    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n    checkpoint_path = os.path.join(MODELS_DIR, f\"{model_name}_best.pth\")\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n        \n        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            \n            if use_mixup:\n                inputs, y_a, y_b, lam = mixup_data(inputs, labels)\n                outputs = model(inputs)\n                loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n            else:\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_loss += loss.item()\n            \n        train_loss /= len(train_loader)\n        history['train_loss'].append(train_loss)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        val_loss /= len(val_loader)\n        val_acc = correct / total if total > 0 else 0\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        \n        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            no_improve_epochs = 0\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'val_acc': val_acc,\n                'val_loss': val_loss,\n            }, checkpoint_path)\n            print(f\"New best model saved with val_acc: {val_acc:.4f}\")\n        else:\n            no_improve_epochs += 1\n        \n        # Early stopping\n        if no_improve_epochs >= patience:\n            print(f\"Early stopping triggered after {epoch+1} epochs\")\n            break\n    \n    # Load best model\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Save training history\n    pd.DataFrame(history).to_csv(os.path.join(LOGS_DIR, f\"{model_name}_history.csv\"), index=False)\n    \n    # Plot training history\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.title('Loss Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['val_acc'], label='Val Accuracy')\n    plt.title('Accuracy Curve')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(VIS_DIR, f\"{model_name}_training_curves.png\"), dpi=300)\n    plt.show()\n    \n    return model, best_val_acc, history\n\ndef train_ensemble(ensemble, train_loader, val_loader, device, epochs=50, patience=10, lr=0.001):\n    \"\"\"\n    Train the adaptive ensemble (condition encoder and weights only)\n    \n    Parameters:\n    -----------\n    ensemble : AdaptiveWeightedEnsemble\n        The ensemble model to train\n    train_loader : DataLoader\n        DataLoader for training data\n    val_loader : DataLoader\n        DataLoader for validation data\n    device : torch.device\n        Device to train on\n    epochs : int, optional\n        Maximum number of epochs to train for\n    patience : int, optional\n        Early stopping patience\n    lr : float, optional\n        Learning rate\n        \n    Returns:\n    --------\n    tuple\n        (trained ensemble, best validation accuracy, training history)\n    \"\"\"\n    ensemble = ensemble.to(device)\n    \n    # Freeze the base models\n    for model in ensemble.models:\n        for param in model.parameters():\n            param.requires_grad = False\n    \n    # Only optimize the condition encoder and base weights\n    params_to_update = list(ensemble.condition_encoder.parameters()) + [ensemble.base_weights]\n    optimizer = optim.Adam(params_to_update, lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    best_val_loss = float('inf')\n    best_val_acc = 0.0\n    no_improve_epochs = 0\n    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n    \n    checkpoint_path = os.path.join(ENSEMBLE_DIR, \"adaptive_ensemble_best.pth\")\n    \n    for epoch in range(epochs):\n        # Training\n        ensemble.train()\n        train_loss = 0.0\n        \n        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            \n            outputs = ensemble(inputs)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            \n        train_loss /= len(train_loader)\n        history['train_loss'].append(train_loss)\n        \n        # Validation\n        ensemble.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = ensemble(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        val_loss /= len(val_loader)\n        val_acc = correct / total if total > 0 else 0\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        \n        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            no_improve_epochs = 0\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': ensemble.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_acc': val_acc,\n                'val_loss': val_loss,\n            }, checkpoint_path)\n            print(f\"New best ensemble saved with val_acc: {val_acc:.4f}\")\n        else:\n            no_improve_epochs += 1\n        \n        # Early stopping\n        if no_improve_epochs >= patience:\n            print(f\"Early stopping triggered after {epoch+1} epochs\")\n            break\n    \n    # Load best model\n    checkpoint = torch.load(checkpoint_path)\n    ensemble.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Save training history\n    pd.DataFrame(history).to_csv(os.path.join(LOGS_DIR, \"adaptive_ensemble_history.csv\"), index=False)\n    \n    # Plot training history\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.title('Loss Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['val_acc'], label='Val Accuracy')\n    plt.title('Accuracy Curve')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(ENSEMBLE_DIR, \"adaptive_ensemble_training_curves.png\"), dpi=300)\n    plt.show()\n    \n    return ensemble, best_val_acc, history\n\nprint(\"âœ… Training functions defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 12: Train Base Models with K-Fold Cross-Validation\n\n# Define the model architectures to use\nmodel_architectures = [\n    'efficientnet_b0',\n    'mobilenetv3_small_100', \n    'resnet18',\n    'mobilenetv2_100',\n    'efficientnet_b1'\n]\n\n# Dictionary to store trained models and accuracies for each fold\nfold_models = {arch: [] for arch in model_architectures}\nfold_val_accuracies = {arch: [] for arch in model_architectures}\nmodel_histories = {arch: [] for arch in model_architectures}\n\n# Train models for each fold\nfor fold_idx in range(K_FOLDS):\n    print(f\"\\n{'='*50}\")\n    print(f\"STARTING FOLD {fold_idx+1}/{K_FOLDS}\")\n    print(f\"{'='*50}\")\n    \n    # Create dataloaders for this fold\n    train_loader, val_loader, label2id = create_fold_dataloaders(fold_idx)\n    \n    for arch in model_architectures:\n        print(f\"\\nðŸ”„ Training model: {arch} for fold {fold_idx+1}/{K_FOLDS}\")\n        \n        # Check if model already exists for this fold\n        checkpoint_path = os.path.join(MODELS_DIR, f\"{arch}_fold{fold_idx}_best.pth\")\n        \n        if os.path.exists(checkpoint_path):\n            print(f\"Loading existing model from {checkpoint_path}\")\n            model = get_base_model(arch, num_classes).to(device)\n            checkpoint = torch.load(checkpoint_path)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            val_acc = checkpoint['val_acc']\n            \n            # Load history if exists\n            history_path = os.path.join(LOGS_DIR, f\"{arch}_fold{fold_idx}_history.csv\")\n            if os.path.exists(history_path):\n                history = pd.read_csv(history_path).to_dict('list')\n            else:\n                history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n        else:\n            model, val_acc, history = train_single_model(\n                model_name=arch,\n                num_classes=num_classes,\n                train_loader=train_loader,\n                val_loader=val_loader,\n                device=device,\n                epochs=EPOCHS_BASE_MODELS,\n                patience=20,  # Increased patience\n                use_mixup=True,\n                label_smoothing=0.1\n            )\n            \n            # Save model with fold information\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'val_acc': val_acc,\n                'fold': fold_idx\n            }, checkpoint_path)\n        \n        # Store the model and accuracy for this fold\n        fold_models[arch].append(model)\n        fold_val_accuracies[arch].append(val_acc)\n        model_histories[arch].append(history)\n        \n        print(f\"âœ… Model {arch} - Fold {fold_idx+1} - Validation accuracy: {val_acc:.4f}\")\n\n# Calculate average validation accuracy across folds\navg_val_accuracies = {arch: np.mean(accs) for arch, accs in fold_val_accuracies.items()}\n\n# Create summary dataframe\nsummary_rows = []\nfor arch in model_architectures:\n    for fold_idx, val_acc in enumerate(fold_val_accuracies[arch]):\n        summary_rows.append({\n            'Architecture': arch,\n            'Fold': fold_idx,\n            'Validation_Accuracy': val_acc\n        })\n\nmodel_summary = pd.DataFrame(summary_rows)\nmodel_summary.to_csv(os.path.join(METRICS_DIR, 'k_fold_base_models_summary.csv'), index=False)\n\n# Add average row for each architecture\navg_summary = pd.DataFrame([{\n    'Architecture': arch,\n    'Fold': 'Average',\n    'Validation_Accuracy': avg_val_accuracies[arch]\n} for arch in model_architectures])\n\nmodel_summary = pd.concat([model_summary, avg_summary])\nmodel_summary.to_csv(os.path.join(METRICS_DIR, 'k_fold_base_models_summary.csv'), index=False)\n\nprint(\"\\nâœ… K-Fold Cross-Validation training complete!\")\nprint(\"\\nAverage validation accuracies across folds:\")\nfor arch, acc in avg_val_accuracies.items():\n    print(f\"{arch}: {acc:.4f}\")\n\n# Visualize average validation accuracies\nplt.figure(figsize=(10, 6))\nbars = plt.bar(model_architectures, [avg_val_accuracies[arch] for arch in model_architectures])\nplt.title('Average Base Model Validation Accuracy Across K-Folds')\nplt.xlabel('Model Architecture')\nplt.ylabel('Average Validation Accuracy')\nplt.xticks(rotation=45)\n\n# Add value annotations\nfor bar in bars:\n    height = bar.get_height()\n    plt.annotate(f'{height:.4f}',\n               xy=(bar.get_x() + bar.get_width() / 2, height),\n               xytext=(0, 3),\n               textcoords=\"offset points\",\n               ha='center', va='bottom')\n\nplt.tight_layout()\nplt.savefig(os.path.join(VIS_DIR, 'k_fold_base_model_accuracies.png'), dpi=300)\nplt.show()\n\n# Select the best model from each architecture (based on validation accuracy)\nbest_models = {}\nbest_fold_idx = {}\n\nfor arch in model_architectures:\n    best_fold = np.argmax(fold_val_accuracies[arch])\n    best_models[arch] = fold_models[arch][best_fold]\n    best_fold_idx[arch] = best_fold\n    print(f\"Best {arch} model from fold {best_fold+1} with accuracy {fold_val_accuracies[arch][best_fold]:.4f}\")\n\n# Use the best models for the ensemble\ntrained_models = [best_models[arch] for arch in model_architectures]\nval_accuracies = [fold_val_accuracies[arch][best_fold_idx[arch]] for arch in model_architectures]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 13: Create and Train Adaptive Weighted Ensemble with K-Fold\n\n# Normalize validation accuracies for initial weights\nbase_weights = np.array(val_accuracies)\nbase_weights = base_weights / base_weights.sum()\n\nprint(f\"Creating ensemble with {len(trained_models)} models\")\nprint(f\"Initial weights based on best validation accuracy: {base_weights}\")\n\n# Create K-Fold ensembles\nfold_ensembles = []\nfold_ensemble_accs = []\n\nfor fold_idx in range(K_FOLDS):\n    print(f\"\\n{'='*50}\")\n    print(f\"TRAINING ENSEMBLE FOR FOLD {fold_idx+1}/{K_FOLDS}\")\n    print(f\"{'='*50}\")\n    \n    # Create dataloaders for this fold\n    train_loader, val_loader, _ = create_fold_dataloaders(fold_idx)\n    \n    # Create the ensemble with the best models from each architecture\n    ensemble = AdaptiveWeightedEnsemble(trained_models, num_classes, base_weights=base_weights)\n    \n    # Check if ensemble already exists\n    checkpoint_path = os.path.join(ENSEMBLE_DIR, f\"adaptive_ensemble_fold{fold_idx}_best.pth\")\n    \n    if os.path.exists(checkpoint_path):\n        print(f\"Loading existing ensemble from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        ensemble.load_state_dict(checkpoint['model_state_dict'])\n        ensemble_val_acc = checkpoint['val_acc']\n    else:\n        # Train the adaptive weights\n        ensemble, ensemble_val_acc, ensemble_history = train_ensemble(\n            ensemble=ensemble,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            device=device,\n            epochs=EPOCHS_ENSEMBLE,\n            patience=10,  # Increased patience\n            lr=0.001\n        )\n        \n        # Save the final model for this fold\n        torch.save({\n            'model_state_dict': ensemble.state_dict(),\n            'base_weights': ensemble.base_weights.detach().cpu().numpy(),\n            'n_models': ensemble.n_models,\n            'n_classes': ensemble.n_classes,\n            'model_architectures': model_architectures,\n            'val_accuracy': ensemble_val_acc,\n            'fold': fold_idx\n        }, checkpoint_path)\n    \n    fold_ensembles.append(ensemble)\n    fold_ensemble_accs.append(ensemble_val_acc)\n    print(f\"âœ… Adaptive Weighted Ensemble for fold {fold_idx+1} - Validation accuracy: {ensemble_val_acc:.4f}\")\n\n# Calculate average ensemble accuracy\navg_ensemble_acc = np.mean(fold_ensemble_accs)\nprint(f\"\\nAverage ensemble validation accuracy: {avg_ensemble_acc:.4f}\")\n\n# Select the best ensemble\nbest_ensemble_idx = np.argmax(fold_ensemble_accs)\nensemble = fold_ensembles[best_ensemble_idx]\nensemble_val_acc = fold_ensemble_accs[best_ensemble_idx]\n\nprint(f\"Best ensemble from fold {best_ensemble_idx+1} with validation accuracy: {ensemble_val_acc:.4f}\")\n\n# Save the final best ensemble\nfinal_save_path = os.path.join(ENSEMBLE_DIR, \"adaptive_ensemble_final_best.pth\")\ntorch.save({\n    'model_state_dict': ensemble.state_dict(),\n    'base_weights': ensemble.base_weights.detach().cpu().numpy(),\n    'n_models': ensemble.n_models,\n    'n_classes': ensemble.n_classes,\n    'model_architectures': model_architectures,\n    'val_accuracy': ensemble_val_acc,\n    'fold': best_ensemble_idx\n}, final_save_path)\n\nprint(f\"Best ensemble saved to {final_save_path}\")\n\n# Visualize ensemble accuracies across folds\nplt.figure(figsize=(8, 6))\nx = np.arange(K_FOLDS)\nplt.bar(x, fold_ensemble_accs)\nplt.axhline(y=avg_ensemble_acc, color='r', linestyle='--', label=f'Average: {avg_ensemble_acc:.4f}')\nplt.xlabel('Fold')\nplt.ylabel('Validation Accuracy')\nplt.title('Ensemble Validation Accuracy Across Folds')\nplt.xticks(x, [f\"Fold {i+1}\" for i in range(K_FOLDS)])\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(os.path.join(ENSEMBLE_DIR, 'ensemble_k_fold_accuracies.png'), dpi=300)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 14: Evaluate Base Models and Ensemble on Test Set\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport json\n\ndef evaluate_model(model, dataloader, device):\n    \"\"\"Evaluate model performance on the given dataloader\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate accuracy\n    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n    \n    return accuracy, all_preds, all_labels\n\ndef plot_confusion_matrix(true_labels, pred_labels, class_names, save_path=None, title=\"Confusion Matrix\"):\n    \"\"\"Plot confusion matrix with proper labels\"\"\"\n    cm = confusion_matrix(true_labels, pred_labels)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title(title)\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    plt.show()\n    \n    # Also plot normalized confusion matrix\n    plt.figure(figsize=(10, 8))\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='YlOrRd', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title(f\"{title} (Normalized)\")\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    \n    if save_path:\n        base, ext = os.path.splitext(save_path)\n        norm_path = f\"{base}_normalized{ext}\"\n        plt.savefig(norm_path, dpi=300, bbox_inches='tight')\n    \n    plt.show()\n\ndef save_classification_report(true_labels, pred_labels, class_names, save_path=None):\n    \"\"\"Generate and save classification report\"\"\"\n    report = classification_report(true_labels, pred_labels, \n                                  target_names=class_names, output_dict=True)\n    \n    if save_path:\n        with open(save_path, 'w') as f:\n            json.dump(report, f, indent=4)\n    \n    return report\n\n# Evaluate each base model\nbase_model_accuracies = []\nfor i, arch in enumerate(model_architectures):\n    model = best_models[arch]\n    model_name = arch\n    print(f\"\\nðŸ“Š Evaluating base model: {model_name}\")\n    accuracy, preds, labels = evaluate_model(model, test_loader, device)\n    base_model_accuracies.append(accuracy)\n    \n    # Save results\n    plot_confusion_matrix(\n        labels, preds, class_names, \n        save_path=os.path.join(VIS_DIR, f\"{model_name}_confusion_matrix.png\"),\n        title=f\"Confusion Matrix - {model_name}\"\n    )\n    \n    report = save_classification_report(\n        labels, preds, class_names,\n        save_path=os.path.join(METRICS_DIR, f\"{model_name}_classification_report.json\")\n    )\n    \n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    print(\"Classification Report:\")\n    print(classification_report(labels, preds, target_names=class_names))\n\n# Evaluate the ensemble\nprint(\"\\nðŸ“Š Evaluating Adaptive Weighted Ensemble\")\nensemble_acc, ensemble_preds, ensemble_labels = evaluate_model(ensemble, test_loader, device)\n\n# Save ensemble results\nplot_confusion_matrix(\n    ensemble_labels, ensemble_preds, class_names, \n    save_path=os.path.join(ENSEMBLE_DIR, \"ensemble_confusion_matrix.png\"),\n    title=\"Confusion Matrix - Adaptive Ensemble\"\n)\n\nensemble_report = save_classification_report(\n    ensemble_labels, ensemble_preds, class_names,\n    save_path=os.path.join(ENSEMBLE_DIR, \"ensemble_classification_report.json\")\n)\n\nprint(f\"Ensemble Test Accuracy: {ensemble_acc:.4f}\")\nprint(\"Ensemble Classification Report:\")\nprint(classification_report(ensemble_labels, ensemble_preds, target_names=class_names))\n\n# Compare models\nall_accuracies = base_model_accuracies + [ensemble_acc]\nall_model_names = model_architectures + ['Adaptive Ensemble']\n\nplt.figure(figsize=(12, 6))\nbars = plt.bar(all_model_names, all_accuracies)\nplt.title('Model Comparison - Test Accuracy')\nplt.xlabel('Model')\nplt.ylabel('Accuracy')\nplt.xticks(rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\n\n# Add value annotations\nfor bar in bars:\n    height = bar.get_height()\n    plt.annotate(f'{height:.4f}',\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3),\n                textcoords=\"offset points\",\n                ha='center', va='bottom')\n\n# Highlight the best model\nbest_idx = np.argmax(all_accuracies)\nbars[best_idx].set_color('green')\n\nplt.savefig(os.path.join(VIS_DIR, \"model_comparison.png\"), dpi=300)\nplt.show()\n\n# Calculate improvement percentage\nbest_base_acc = max(base_model_accuracies)\nimprovement = (ensemble_acc - best_base_acc) / best_base_acc * 100\n\nprint(f\"\\nBest base model accuracy: {best_base_acc:.4f}\")\nprint(f\"Ensemble accuracy: {ensemble_acc:.4f}\")\nprint(f\"Improvement: {improvement:.2f}%\")\n\n# Save comparison results\ncomparison_results = {\n    'base_models': {model_architectures[i]: base_model_accuracies[i] for i in range(len(model_architectures))},\n    'ensemble': ensemble_acc,\n    'improvement': improvement\n}\n\nwith open(os.path.join(METRICS_DIR, 'model_comparison.json'), 'w') as f:\n    json.dump(comparison_results, f, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 15: Visualize Adaptive Weights\n\ndef visualize_adaptive_weights(ensemble, test_loader, device, num_samples=6, save_dir=None):\n    \"\"\"Visualize how the model adapts weights for different water conditions\"\"\"\n    ensemble.eval()\n    # Get a batch of images\n    images, labels = next(iter(test_loader))\n    images = images[:num_samples].to(device)\n    labels = labels[:num_samples].to(device)\n    \n    # Get base weights\n    base_weights = ensemble.base_weights.detach().cpu().numpy()\n    \n    # Get condition-specific weights\n    with torch.no_grad():\n        condition_weights = ensemble.condition_encoder(images).cpu().numpy()\n    \n    # Calculate final weights\n    final_weights = []\n    for i in range(num_samples):\n        sample_weights = base_weights * condition_weights[i]\n        sample_weights = sample_weights / sample_weights.sum()\n        final_weights.append(sample_weights)\n    \n    # Plot\n    fig, axes = plt.subplots(num_samples, 2, figsize=(14, 3*num_samples))\n    model_names = [f\"Model {i+1}\" for i in range(len(ensemble.models))]\n    \n    for i in range(num_samples):\n        # Plot the image\n        img = images[i].cpu().permute(1, 2, 0).numpy()\n        # Normalize for display\n        img = (img - img.min()) / (img.max() - img.min())\n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title(f\"Sample {i+1}\")\n        axes[i, 0].axis('off')\n        \n        # Plot the weights\n        bars = axes[i, 1].bar(model_names, final_weights[i])\n        axes[i, 1].set_ylim(0, 0.5)\n        axes[i, 1].set_title(f\"Model Weights for Sample {i+1}\")\n        \n        # Add value annotations\n        for bar in bars:\n            height = bar.get_height()\n            axes[i, 1].annotate(f'{height:.3f}',\n                               xy=(bar.get_x() + bar.get_width() / 2, height),\n                               xytext=(0, 3),\n                               textcoords=\"offset points\",\n                               ha='center', va='bottom')\n    \n    plt.tight_layout()\n    \n    if save_dir:\n        save_path = os.path.join(save_dir, \"adaptive_weights_visualization.png\")\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    plt.show()\n    \n    # Also visualize the learned base weights\n    plt.figure(figsize=(10, 6))\n    bars = plt.bar(model_architectures, base_weights)\n    plt.title('Base Model Weights in Adaptive Ensemble')\n    plt.xlabel('Model Architecture')\n    plt.ylabel('Weight')\n    plt.xticks(rotation=45)\n    \n    for bar in bars:\n        height = bar.get_height()\n        plt.annotate(f'{height:.3f}',\n                   xy=(bar.get_x() + bar.get_width() / 2, height),\n                   xytext=(0, 3),\n                   textcoords=\"offset points\",\n                   ha='center', va='bottom')\n    \n    plt.tight_layout()\n    \n    if save_dir:\n        save_path = os.path.join(save_dir, \"base_weights.png\")\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    plt.show()\n\nprint(\"Visualizing adaptive weights for different water conditions...\")\nvisualize_adaptive_weights(\n    ensemble=ensemble,\n    test_loader=test_loader,\n    device=device,\n    num_samples=6,\n    save_dir=ENSEMBLE_DIR\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 16: Ablation Study for Adaptive Weighting\n\ndef run_ablation_study(models, ensemble, test_loader, device, class_names):\n    \"\"\"Compare different ensemble methods: majority vote, average, and adaptive weights\"\"\"\n    # Get all test samples\n    all_inputs = []\n    all_labels = []\n    \n    for inputs, labels in tqdm(test_loader, desc=\"Collecting test data\"):\n        all_inputs.append(inputs)\n        all_labels.append(labels)\n    \n    test_inputs = torch.cat(all_inputs)\n    test_labels = torch.cat(all_labels)\n    \n    # Results containers\n    results = {\n        'per_model': [],\n        'majority_vote': None,\n        'simple_average': None,\n        'weighted_average': None,\n        'adaptive_weights': None\n    }\n    \n    # Get predictions from individual models\n    print(\"Evaluating individual models...\")\n    all_model_outputs = []\n    \n    with torch.no_grad():\n        for i, model in enumerate(models):\n            model_name = model_architectures[i]\n            print(f\"  Processing {model_name}...\")\n            \n            # Get model predictions\n            model_outputs = []\n            for inputs in tqdm(all_inputs, desc=f\"Processing {model_name}\", leave=False):\n                inputs = inputs.to(device)\n                outputs = model(inputs)\n                model_outputs.append(outputs)\n            \n            model_outputs = torch.cat(model_outputs)\n            all_model_outputs.append(model_outputs)\n            \n            # Calculate accuracy\n            preds = model_outputs.argmax(dim=1)\n            accuracy = (preds == test_labels.to(device)).float().mean().item()\n            results['per_model'].append(accuracy)\n    \n    # Stack all outputs: [n_models, n_samples, n_classes]\n    all_model_outputs = torch.stack(all_model_outputs)\n    \n    # 1. Majority Voting\n    print(\"Evaluating majority voting...\")\n    model_preds = all_model_outputs.argmax(dim=2)  # [n_models, n_samples]\n    vote_counts = torch.zeros(test_labels.size(0), num_classes, device=device)\n    \n    for i in range(len(models)):\n        for j in range(test_labels.size(0)):\n            vote_counts[j, model_preds[i, j]] += 1\n    \n    majority_preds = vote_counts.argmax(dim=1)\n    majority_acc = (majority_preds == test_labels.to(device)).float().mean().item()\n    results['majority_vote'] = majority_acc\n    \n    # 2. Simple Average (Equal weights)\n    print(\"Evaluating simple average...\")\n    avg_outputs = all_model_outputs.mean(dim=0)  # [n_samples, n_classes]\n    avg_preds = avg_outputs.argmax(dim=1)\n    avg_acc = (avg_preds == test_labels.to(device)).float().mean().item()\n    results['simple_average'] = avg_acc\n    \n    # 3. Weighted Average (Based on validation accuracy)\n    print(\"Evaluating weighted average...\")\n    val_weights = torch.tensor(val_accuracies, device=device)\n    val_weights = val_weights / val_weights.sum()\n    \n    weighted_outputs = (all_model_outputs * val_weights.view(-1, 1, 1)).sum(dim=0)\n    weighted_preds = weighted_outputs.argmax(dim=1)\n    weighted_acc = (weighted_preds == test_labels.to(device)).float().mean().item()\n    results['weighted_average'] = weighted_acc\n    \n    # 4. Adaptive Weighted Ensemble\n    print(\"Evaluating adaptive weighted ensemble...\")\n    with torch.no_grad():\n        ensemble_outputs = []\n        for inputs in tqdm(all_inputs, desc=\"Processing ensemble\", leave=False):\n            inputs = inputs.to(device)\n            outputs = ensemble(inputs)\n            ensemble_outputs.append(outputs)\n        \n        ensemble_outputs = torch.cat(ensemble_outputs)\n        ensemble_preds = ensemble_outputs.argmax(dim=1)\n        adaptive_acc = (ensemble_preds == test_labels.to(device)).float().mean().item()\n        results['adaptive_weights'] = adaptive_acc\n    \n    # Visualize results\n    ablation_methods = ['Individual Models (Avg)', 'Majority Vote', 'Simple Average', \n                      'Weighted Average', 'Adaptive Ensemble']\n    ablation_accs = [sum(results['per_model'])/len(results['per_model']), \n                   results['majority_vote'], \n                   results['simple_average'],\n                   results['weighted_average'], \n                   results['adaptive_weights']]\n    \n    plt.figure(figsize=(12, 6))\n    bars = plt.bar(ablation_methods, ablation_accs, color=['lightgray', 'lightblue', \n                                                        'lightgreen', 'orange', 'red'])\n    \n    # Add value annotations\n    for bar in bars:\n        height = bar.get_height()\n        plt.annotate(f'{height:.4f}',\n                   xy=(bar.get_x() + bar.get_width() / 2, height),\n                   xytext=(0, 3),\n                   textcoords=\"offset points\",\n                   ha='center', va='bottom')\n    \n    plt.title('Ablation Study: Comparing Ensemble Methods')\n    plt.xlabel('Method')\n    plt.ylabel('Accuracy')\n    plt.ylim([min(ablation_accs) - 0.05, max(ablation_accs) + 0.05])\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    \n    plt.savefig(os.path.join(ENSEMBLE_DIR, 'ablation_study.png'), dpi=300)\n    plt.show()\n    \n    # Save ablation results\n    ablation_df = pd.DataFrame({\n        'Method': ablation_methods,\n        'Accuracy': ablation_accs\n    })\n    ablation_df.to_csv(os.path.join(METRICS_DIR, 'ablation_results.csv'), index=False)\n    \n    # Also save individual model results\n    model_df = pd.DataFrame({\n        'Model': model_architectures,\n        'Accuracy': results['per_model']\n    })\n    model_df.to_csv(os.path.join(METRICS_DIR, 'individual_model_results.csv'), index=False)\n    \n    print(\"âœ… Ablation study completed\")\n    print(\"\\nComparison of ensemble methods:\")\n    for method, acc in zip(ablation_methods, ablation_accs):\n        print(f\"{method}: {acc:.4f}\")\n\n# Run ablation study\nrun_ablation_study(\n    models=trained_models,\n    ensemble=ensemble,\n    test_loader=test_loader,\n    device=device,\n    class_names=class_names\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 17: Per-Class Performance Analysis\n\nfrom sklearn.metrics import precision_recall_fscore_support\n\ndef analyze_per_class_performance(models, ensemble, test_loader, device, class_names):\n    \"\"\"Generate and visualize per-class performance metrics\"\"\"\n    # Get predictions from all models\n    model_results = []\n    ensemble_preds = None\n    true_labels = None\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=\"Evaluating models\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Get individual model predictions\n            batch_preds = []\n            for model in models:\n                outputs = model(inputs)\n                preds = outputs.argmax(dim=1)\n                batch_preds.append(preds.cpu().numpy())\n            \n            # Get ensemble predictions\n            ensemble_outputs = ensemble(inputs)\n            ensemble_batch_preds = ensemble_outputs.argmax(dim=1).cpu().numpy()\n            \n            # Store results\n            if ensemble_preds is None:\n                ensemble_preds = ensemble_batch_preds\n                true_labels = labels.cpu().numpy()\n                model_results = [p for p in batch_preds]\n            else:\n                ensemble_preds = np.concatenate([ensemble_preds, ensemble_batch_preds])\n                true_labels = np.concatenate([true_labels, labels.cpu().numpy()])\n                for i, p in enumerate(batch_preds):\n                    model_results[i] = np.concatenate([model_results[i], p])\n    \n    # Calculate per-class metrics\n    all_precisions = []\n    all_recalls = []\n    all_f1s = []\n    \n    # Individual models\n    for i, preds in enumerate(model_results):\n        precision, recall, f1, _ = precision_recall_fscore_support(\n            true_labels, preds, average=None, zero_division=0\n        )\n        all_precisions.append(precision)\n        all_recalls.append(recall)\n        all_f1s.append(f1)\n    \n    # Ensemble\n    ens_precision, ens_recall, ens_f1, support = precision_recall_fscore_support(\n        true_labels, ensemble_preds, average=None, zero_division=0\n    )\n    all_precisions.append(ens_precision)\n    all_recalls.append(ens_recall)\n    all_f1s.append(ens_f1)\n    \n    # Convert to arrays for easier plotting\n    all_precisions = np.array(all_precisions)\n    all_recalls = np.array(all_recalls)\n    all_f1s = np.array(all_f1s)\n    \n    # Plotting\n    metrics = ['Precision', 'Recall', 'F1-Score']\n    all_metrics = [all_precisions, all_recalls, all_f1s]\n    \n    for metric_idx, (metric_name, metric_data) in enumerate(zip(metrics, all_metrics)):\n        plt.figure(figsize=(12, 8))\n        \n        x = np.arange(len(class_names))\n        width = 0.8 / (len(models) + 1)  # Bar width\n        \n        for i in range(len(models)):\n            plt.bar(x + i*width - 0.4, metric_data[i], width, label=model_architectures[i])\n        \n        # Ensemble bars with distinctive color\n        plt.bar(x + len(models)*width - 0.4, metric_data[-1], width, label='Adaptive Ensemble', color='red')\n        \n        plt.xlabel('Class')\n        plt.ylabel(metric_name)\n        plt.title(f'Per-Class {metric_name}')\n        plt.xticks(x, class_names, rotation=45)\n        plt.legend()\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        \n        plt.savefig(os.path.join(METRICS_DIR, f'per_class_{metric_name.lower()}.png'), dpi=300)\n        plt.show()\n    \n    # Additional visualization: Class support (number of samples)\n    plt.figure(figsize=(10, 6))\n    plt.bar(class_names, support)\n    plt.xlabel('Class')\n    plt.ylabel('Number of Test Samples')\n    plt.title('Class Distribution in Test Set')\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig(os.path.join(METRICS_DIR, 'test_class_distribution.png'), dpi=300)\n    plt.show()\n    \n    # Save metrics as CSV\n    results_df = pd.DataFrame()\n    for i, model_name in enumerate(model_architectures + ['Adaptive Ensemble']):\n        for cls_idx, cls_name in enumerate(class_names):\n            results_df = pd.concat([results_df, pd.DataFrame({\n                'Model': [model_name],\n                'Class': [cls_name],\n                'Precision': [all_precisions[i, cls_idx]],\n                'Recall': [all_recalls[i, cls_idx]],\n                'F1-Score': [all_f1s[i, cls_idx]],\n                'Support': [support[cls_idx]]\n            })])\n    \n    results_df.to_csv(os.path.join(METRICS_DIR, 'per_class_metrics.csv'), index=False)\n    print(\"âœ… Per-class performance analysis completed and saved\")\n\n# Run per-class analysis\nanalyze_per_class_performance(\n    models=trained_models,\n    ensemble=ensemble,\n    test_loader=test_loader,\n    device=device,\n    class_names=class_names\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 18: Interpretability Analysis with Grad-CAM\n\nfrom pytorch_grad_cam import GradCAM, ScoreCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\ndef get_last_conv_layer(model):\n    \"\"\"Find the last convolutional layer in a model\"\"\"\n    target_layer = None\n    # Common patterns for different architectures\n    if hasattr(model, 'features') and hasattr(model.features[-1], 'weight'):\n        # EfficientNet, MobileNet, etc.\n        return model.features[-1]\n    \n    # For ResNet\n    if hasattr(model, 'layer4'):\n        return model.layer4[-1]\n    \n    # Generic search - find last conv layer\n    for name, module in reversed(list(model.named_modules())):\n        if isinstance(module, torch.nn.Conv2d):\n            return module\n    \n    raise ValueError(\"Could not find a convolutional layer for CAM\")\n\ndef visualize_gradcam_comparison(models, ensemble, test_loader, device, class_names, n_samples=5):\n    \"\"\"Visualize GradCAM for multiple models and the ensemble on same images\"\"\"\n    # Get some test samples\n    all_samples = []\n    for inputs, labels in test_loader:\n        batch_samples = [(img, label) for img, label in zip(inputs, labels)]\n        all_samples.extend(batch_samples)\n        if len(all_samples) >= n_samples:\n            break\n    \n    samples = all_samples[:n_samples]\n    \n    for sample_idx, (image, true_label) in enumerate(samples):\n        fig, axes = plt.subplots(2, len(models) + 1, figsize=(16, 8))\n        \n        # Original image\n        img_np = image.permute(1, 2, 0).numpy()\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img_np = img_np * std + mean\n        img_np = np.clip(img_np, 0, 1)\n        \n        # Process each base model\n        all_preds = []\n        for i, model in enumerate(models):\n            model.eval()\n            \n            # Get prediction\n            input_tensor = image.unsqueeze(0).to(device)\n            with torch.no_grad():\n                output = model(input_tensor)\n                pred = output.argmax(dim=1).item()\n            all_preds.append(pred)\n            \n            # GradCAM\n            target_layer = get_last_conv_layer(model)\n            cam = GradCAM(model=model, target_layers=[target_layer])\n            targets = [ClassifierOutputTarget(pred)]\n            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n            cam_image = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n            \n            # Display original\n            if sample_idx == 0:\n                axes[0, i].set_title(f\"{model_architectures[i]}\\nPrediction: {class_names[pred]}\")\n            else:\n                axes[0, i].set_title(f\"Prediction: {class_names[pred]}\")\n            axes[0, i].imshow(img_np)\n            axes[0, i].axis('off')\n            \n            # Display GradCAM\n            axes[1, i].imshow(cam_image)\n            axes[1, i].axis('off')\n            axes[1, i].set_title(\"Attention Map\")\n        \n        # Process ensemble\n        ensemble.eval()\n        input_tensor = image.unsqueeze(0).to(device)\n        with torch.no_grad():\n            output = ensemble(input_tensor)\n            ensemble_pred = output.argmax(dim=1).item()\n        \n        # For ensemble, use ScoreCAM (doesn't need gradients)\n        target_layer = ensemble.condition_encoder[0]  # First conv layer of condition encoder\n        cam = ScoreCAM(model=ensemble, target_layers=[target_layer])\n        targets = [ClassifierOutputTarget(ensemble_pred)]\n        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n        cam_image = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n        \n        # Display ensemble results\n        if sample_idx == 0:\n            axes[0, -1].set_title(f\"Adaptive Ensemble\\nPrediction: {class_names[ensemble_pred]}\")\n        else:\n            axes[0, -1].set_title(f\"Prediction: {class_names[ensemble_pred]}\")\n        axes[0, -1].imshow(img_np)\n        axes[0, -1].axis('off')\n        axes[1, -1].imshow(cam_image)\n        axes[1, -1].axis('off')\n        axes[1, -1].set_title(\"Attention Map\")\n        \n        # Global title\n        correct_txt = \"CORRECT\" if ensemble_pred == true_label.item() else \"INCORRECT\"\n        fig.suptitle(f\"Sample {sample_idx+1} | True: {class_names[true_label.item()]} | Ensemble: {correct_txt}\", \n                    fontsize=16)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(INTERP_DIR, f\"gradcam_comparison_{sample_idx+1}.png\"), dpi=300)\n        plt.show()\n        \n    print(\"âœ… GradCAM visualizations generated and saved to interpretability directory\")\n\n# Generate GradCAM visualizations\nvisualize_gradcam_comparison(\n    models=trained_models,\n    ensemble=ensemble,\n    test_loader=test_loader,\n    device=device,\n    class_names=class_names,\n    n_samples=5\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 19: Failure Analysis - Find cases where ensemble succeeds but individual models fail\n\ndef analyze_model_failures(models, ensemble, test_loader, device, class_names, n_samples=5):\n    \"\"\"Find and visualize cases where ensemble succeeds but individual models fail\"\"\"\n    # Collect samples where ensemble is correct but some models are wrong\n    success_cases = []\n    \n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        batch_size = inputs.size(0)\n        \n        # Get ensemble predictions\n        with torch.no_grad():\n            ensemble_outputs = ensemble(inputs)\n            ensemble_preds = ensemble_outputs.argmax(dim=1)\n            \n            # Only consider cases where ensemble is correct\n            correct_mask = (ensemble_preds == labels)\n            if not correct_mask.any():\n                continue\n                \n            # Get individual model predictions\n            model_preds = []\n            for model in models:\n                outputs = model(inputs)\n                preds = outputs.argmax(dim=1)\n                model_preds.append(preds)\n            \n            model_preds = torch.stack(model_preds)  # [n_models, batch_size]\n            \n            # Find cases where at least one model is wrong\n            for i in range(batch_size):\n                if correct_mask[i]:\n                    # Check if any model is wrong\n                    any_wrong = False\n                    for j in range(len(models)):\n                        if model_preds[j, i] != labels[i]:\n                            any_wrong = True\n                            break\n                    \n                    if any_wrong:\n                        success_cases.append((\n                            inputs[i].cpu(),\n                            labels[i].item(),\n                            ensemble_preds[i].item(),\n                            [mp[i].item() for mp in model_preds]\n                        ))\n                        \n                        if len(success_cases) >= n_samples:\n                            break\n        \n        if len(success_cases) >= n_samples:\n            break\n    \n    # Visualize the success cases\n    if not success_cases:\n        print(\"â— Could not find cases where ensemble succeeds but some models fail.\")\n        return\n    \n    for idx, (img, true_label, ensemble_pred, model_preds) in enumerate(success_cases):\n        # Convert image for display\n        img_np = img.permute(1, 2, 0).numpy()\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img_np = img_np * std + mean\n        img_np = np.clip(img_np, 0, 1)\n        \n        # Create figure\n        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n        ax.imshow(img_np)\n        ax.set_title(f\"True Class: {class_names[true_label]}\", fontsize=14, pad=20)\n        ax.axis('off')\n        \n        # Create table of predictions\n        model_names = model_architectures + ['Adaptive Ensemble']\n        all_preds = model_preds + [ensemble_pred]\n        correct = [pred == true_label for pred in all_preds]\n        \n        cell_text = []\n        for i, (name, pred, is_correct) in enumerate(zip(model_names, all_preds, correct)):\n            pred_name = class_names[pred]\n            status = \"âœ“\" if is_correct else \"âœ—\"\n            cell_text.append([name, pred_name, status])\n        \n        # Add table below the image\n        table = plt.table(cellText=cell_text,\n                          colLabels=['Model', 'Prediction', 'Correct?'],\n                          loc='bottom', bbox=[0, -0.5, 1, 0.3])\n        table.auto_set_font_size(False)\n        table.set_fontsize(10)\n        table.scale(1, 1.5)\n        \n        # Color the rows based on correctness\n        for i in range(len(all_preds)):\n            if correct[i]:\n                table[(i+1, 2)].set_facecolor('#d8f3dc')  # light green\n            else:\n                table[(i+1, 2)].set_facecolor('#ffccd5')  # light red\n        \n        plt.tight_layout()\n        \n        plt.savefig(os.path.join(INTERP_DIR, f\"failure_analysis_{idx+1}.png\"), dpi=300, bbox_inches='tight')\n        plt.show()\n    \n    print(f\"âœ… Failure analysis visualizations generated for {len(success_cases)} samples\")\n\n# Run failure analysis\nanalyze_model_failures(\n    models=trained_models,\n    ensemble=ensemble,\n    test_loader=test_loader,\n    device=device,\n    class_names=class_names,\n    n_samples=5\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 20: Condition-Based Analysis for Different Water Types\n\ndef analyze_water_conditions(ensemble, test_loader, device, class_names, n_samples=10):\n    \"\"\"Analyze how adaptive weights change across different water conditions\"\"\"\n    # Collect diverse water samples\n    water_samples = []\n    water_labels = []\n    \n    for inputs, labels in test_loader:\n        for img, label in zip(inputs, labels):\n            # Simple diversity check - look at average color to categorize water types\n            # This is a simplification - in practice you might use domain knowledge\n            img_np = img.numpy().transpose(1, 2, 0)\n            avg_color = img_np.mean(axis=(0, 1))\n            brightness = avg_color.mean()\n            \n            # Collect samples with different brightness (proxy for water condition)\n            if len(water_samples) < n_samples:\n                water_samples.append(img)\n                water_labels.append((label.item(), brightness))\n            else:\n                # Replace sample to maximize diversity\n                brightness_values = [b for _, b in water_labels]\n                if abs(brightness - np.mean(brightness_values)) > abs(brightness_values[0] - np.mean(brightness_values)):\n                    water_samples[0] = img\n                    water_labels[0] = (label.item(), brightness)\n                \n                # Sort by brightness\n                indices = sorted(range(len(water_labels)), key=lambda i: water_labels[i][1])\n                water_samples = [water_samples[i] for i in indices]\n                water_labels = [water_labels[i] for i in indices]\n    \n    # Analyze adaptive weighting on diverse samples\n    ensemble.eval()\n    \n    plt.figure(figsize=(15, 10))\n    \n    for i, (img, (label, brightness)) in enumerate(zip(water_samples, water_labels)):\n        # Get model weights for this sample\n        img_tensor = img.unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            # Get condition-specific weights\n            condition_weights = ensemble.condition_encoder(img_tensor).cpu().numpy()[0]\n            \n            # Get base weights\n            base_weights = ensemble.base_weights.detach().cpu().numpy()\n            \n            # Calculate final weights\n            final_weights = base_weights * condition_weights\n            final_weights = final_weights / final_weights.sum()\n            \n            # Get prediction\n            outputs = ensemble(img_tensor)\n            pred = outputs.argmax(dim=1).item()\n        \n        # Create plot in grid\n        plt.subplot(3, 4, i+1)\n        \n        # Show image\n        img_np = img.permute(1, 2, 0).numpy()\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img_np = img_np * std + mean\n        img_np = np.clip(img_np, 0, 1)\n        \n        # Show image in top 70% of subplot\n        plt.imshow(img_np)\n        plt.title(f\"Water Type {i+1}: {class_names[pred]}\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(ENSEMBLE_DIR, 'water_types.png'), dpi=300)\n    plt.show()\n    \n    # Create weight comparison plot as heatmap\n    plt.figure(figsize=(15, 10))\n    \n    # Get all weights in a 2D array [n_samples, n_models]\n    all_weights = np.zeros((len(water_samples), len(model_architectures)))\n    \n    for i, (img, _) in enumerate(zip(water_samples, water_labels)):\n        img_tensor = img.unsqueeze(0).to(device)\n        with torch.no_grad():\n            condition_weights = ensemble.condition_encoder(img_tensor).cpu().numpy()[0]\n            base_weights = ensemble.base_weights.detach().cpu().numpy()\n            final_weights = base_weights * condition_weights\n            final_weights = final_weights / final_weights.sum()\n            all_weights[i] = final_weights\n    \n    # Create heatmap\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(all_weights, annot=True, fmt=\".3f\", cmap=\"YlGnBu\",\n                xticklabels=model_architectures,\n                yticklabels=[f\"Water Type {i+1}\" for i in range(len(water_samples))])\n    plt.title(\"Adaptive Weight Distribution Across Water Types\")\n    plt.xlabel(\"Model\")\n    plt.ylabel(\"Water Sample\")\n    plt.tight_layout()\n    \n    plt.savefig(os.path.join(ENSEMBLE_DIR, 'adaptive_weights_heatmap.png'), dpi=300)\n    plt.show()\n    \n    print(\"âœ… Water condition analysis completed\")\n\n# Run water condition analysis\nanalyze_water_conditions(\n    ensemble=ensemble,\n    test_loader=test_loader,\n    device=device,\n    class_names=class_names,\n    n_samples=12  # 3x4 grid\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 21: Generate Summary Statistics and Final Journal-Quality Visualizations\n\n# 1. Create overall summary table\nsummary_dict = {\n    'Metric': [],\n    'Value': []\n}\n\n# Basic dataset statistics\nsummary_dict['Metric'].append('Total Classes')\nsummary_dict['Value'].append(num_classes)\n\nsummary_dict['Metric'].append('Total Samples')\ntest_total = len(test_ds)\ntrain_val_total = sum(sum(cls.values()) for cls in [split_counts['train']] + [fc['train'] for fc in fold_counts]) + \\\n                 sum(sum(cls.values()) for cls in [split_counts['val']] + [fc['val'] for fc in fold_counts])\nsummary_dict['Value'].append(test_total + train_val_total)\n\nsummary_dict['Metric'].append('K-Fold Cross Validation')\nsummary_dict['Value'].append(f\"{K_FOLDS}-fold\")\n\nsummary_dict['Metric'].append('Base Models')\nsummary_dict['Value'].append(len(model_architectures))\n\n# Model performance\nsummary_dict['Metric'].append('Best Base Model')\nbest_idx = np.argmax(base_model_accuracies)\nsummary_dict['Value'].append(f\"{model_architectures[best_idx]} ({base_model_accuracies[best_idx]:.4f})\")\n\nsummary_dict['Metric'].append('Ensemble Accuracy')\nsummary_dict['Value'].append(f\"{ensemble_acc:.4f}\")\n\nsummary_dict['Metric'].append('Accuracy Improvement')\nsummary_dict['Value'].append(f\"{improvement:.2f}%\")\n\n# Ablation results if available\nif 'ablation_df' in locals():\n    summary_dict['Metric'].append('Simple Average Accuracy')\n    simple_avg_acc = ablation_df.loc[ablation_df['Method'] == 'Simple Average', 'Accuracy'].values[0]\n    summary_dict['Value'].append(f\"{simple_avg_acc:.4f}\")\n    \n    summary_dict['Metric'].append('Majority Vote Accuracy')\n    majority_acc = ablation_df.loc[ablation_df['Method'] == 'Majority Vote', 'Accuracy'].values[0]\n    summary_dict['Value'].append(f\"{majority_acc:.4f}\")\n\n# Save summary\nsummary_df = pd.DataFrame(summary_dict)\nsummary_df.to_csv(os.path.join(METRICS_DIR, 'overall_summary.csv'), index=False)\n\n# 2. Create publication-quality comparison chart with K-fold information\nplt.figure(figsize=(14, 8))\n\n# First show fold accuracies for each model\nx_positions = []\nx_labels = []\nfor i, arch in enumerate(model_architectures):\n    fold_accs = fold_val_accuracies[arch]\n    x_pos = np.arange(K_FOLDS) + i * (K_FOLDS + 2)\n    plt.bar(x_pos, fold_accs, width=0.8, alpha=0.6, label=f\"{arch} folds\" if i==0 else \"\")\n    x_positions.extend(x_pos)\n    x_labels.extend([f\"F{j+1}\" for j in range(K_FOLDS)])\n    \n    # Add average for this model\n    avg_pos = x_pos[-1] + 1\n    avg_acc = np.mean(fold_accs)\n    plt.bar(avg_pos, avg_acc, width=0.8, color='black', alpha=0.8, label=f\"Average\" if i==0 else \"\")\n    x_positions.append(avg_pos)\n    x_labels.append(\"Avg\")\n\n# Then show ensemble accuracies\nens_pos = x_positions[-1] + 3\nplt.bar(ens_pos, ensemble_acc, width=0.8, color='red', label=f\"Adaptive Ensemble\")\nplt.axhline(y=ensemble_acc, color='red', linestyle='--', alpha=0.5)\nx_positions.append(ens_pos)\nx_labels.append(\"Ensemble\")\n\nplt.xticks(x_positions, x_labels, rotation=45)\nplt.title('K-Fold Cross-Validation Results', fontsize=16)\nplt.xlabel('Models and Folds', fontsize=14)\nplt.ylabel('Accuracy', fontsize=14)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\n\nplt.savefig(os.path.join(VIS_DIR, \"k_fold_comparison.png\"), dpi=300, bbox_inches='tight')\nplt.show()\n\n# 3. Create final model comparison with error bars from cross-validation\nplt.figure(figsize=(10, 6))\n\n# Get mean and std for each model\nmeans = [np.mean(fold_val_accuracies[arch]) for arch in model_architectures]\nstds = [np.std(fold_val_accuracies[arch]) for arch in model_architectures]\n\n# Add ensemble result (use a small error bar for visual consistency)\nall_models = model_architectures + ['Adaptive Ensemble']\nall_means = means + [ensemble_acc]\nall_stds = stds + [0.01]  # Small error for ensemble\n\n# Plot with error bars\nbars = plt.bar(range(len(all_models)), all_means, yerr=all_stds, \n         capsize=5, color=['blue']*len(model_architectures) + ['red'])\n\n# Add value annotations\nfor i, bar in enumerate(bars):\n    height = bar.get_height()\n    plt.annotate(f'{all_means[i]:.4f}',\n               xy=(bar.get_x() + bar.get_width() / 2, height),\n               xytext=(0, 3),\n               textcoords=\"offset points\",\n               ha='center', va='bottom')\n\nplt.xticks(range(len(all_models)), all_models, rotation=45)\nplt.title('Model Performance Comparison with K-Fold Cross-Validation', fontsize=16)\nplt.xlabel('Model', fontsize=14)\nplt.ylabel('Accuracy', fontsize=14)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\n\nplt.savefig(os.path.join(VIS_DIR, \"final_model_comparison.png\"), dpi=300, bbox_inches='tight')\nplt.show()\n\n# Display summary\nprint(\"\\nðŸ“Š Final Model Summary\")\nprint(\"=\" * 80)\nprint(f\"Dataset: {num_classes} classes, {train_val_total + test_total} total samples\")\nprint(f\"Cross-validation: {K_FOLDS}-fold with {EPOCHS_BASE_MODELS} max epochs\")\nprint(f\"Best Base Model: {model_architectures[np.argmax(base_model_accuracies)]} - {max(base_model_accuracies):.4f}\")\nprint(f\"Adaptive Ensemble: {ensemble_acc:.4f} ({improvement:.2f}% improvement)\")\nprint(\"=\" * 80)\nprint(\"\\nSee visualization and result directories for all charts and metrics.\")\nprint(f\"  - Models: {MODELS_DIR}\")\nprint(f\"  - Metrics: {METRICS_DIR}\")\nprint(f\"  - Visualizations: {VIS_DIR}\")\nprint(f\"  - Ensemble Analysis: {ENSEMBLE_DIR}\")\nprint(f\"  - Interpretability: {INTERP_DIR}\")\n\nprint(\"\\nâœ… Adaptive Weighted Ensemble implementation with K-fold cross-validation complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\n\n# Define the folders to zip individually\nfolders = [\n    \"ensemble\",\n    \"features\",\n    \"interpretability\",\n    \"logs\",\n    \"metrics\",\n    \"models\",\n    \"splits\",\n    \"visualizations\"\n]\n\nprint(\"ðŸ”„ Starting to zip individual folders...\\n\")\n\n# Process each folder\nfor folder in folders:\n    folder_path = f\"/kaggle/working/outputs/{folder}\"\n    zip_path = f\"/kaggle/working/{folder}.zip\"\n    \n    # Check if folder exists\n    if not os.path.exists(folder_path):\n        print(f\"âš ï¸ Warning: Folder {folder_path} not found. Skipping.\")\n        continue\n    \n    # Zip the folder\n    print(f\"ðŸ”„ Zipping {folder}...\")\n    start_time = time.time()\n    !zip -r {zip_path} {folder_path}\n    \n    # Get zip file size and completion time\n    if os.path.exists(zip_path):\n        size_mb = os.path.getsize(zip_path) / (1024*1024)\n        elapsed = time.time() - start_time\n        print(f\"âœ… Created {folder}.zip: {size_mb:.2f} MB (in {elapsed:.1f} seconds)\")\n    else:\n        print(f\"âŒ Failed to create {folder}.zip\")\n\nprint(\"\\nâœ… All zipping completed!\")\nprint(\"\\nZip files created in /kaggle/working/:\")\n!ls -lh /kaggle/working/*.zip | sort -k5 -h\n\nprint(\"\\nTo download these files manually:\")\nprint(\"1. Look for the zip files in the file browser on the right side\")\nprint(\"2. Click the three dots (â‹®) next to each file\")\nprint(\"3. Select 'Download' from the dropdown menu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}