{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:04:54.936810Z",
     "iopub.status.busy": "2025-05-23T17:04:54.936538Z",
     "iopub.status.idle": "2025-05-23T17:04:56.547783Z",
     "shell.execute_reply": "2025-05-23T17:04:56.546990Z",
     "shell.execute_reply.started": "2025-05-23T17:04:54.936787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 0: Set Global Seed for Reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:04:56.549459Z",
     "iopub.status.busy": "2025-05-23T17:04:56.548987Z",
     "iopub.status.idle": "2025-05-23T17:05:06.126833Z",
     "shell.execute_reply": "2025-05-23T17:05:06.126090Z",
     "shell.execute_reply.started": "2025-05-23T17:04:56.549432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Install essential libraries for training, evaluation, and model analysis\n",
    "!pip install torch torchvision\n",
    "!pip install timm transformers roboflow pycocotools\n",
    "!pip install grad-cam torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:05:06.128763Z",
     "iopub.status.busy": "2025-05-23T17:05:06.127945Z",
     "iopub.status.idle": "2025-05-23T17:05:07.747862Z",
     "shell.execute_reply": "2025-05-23T17:05:07.747284Z",
     "shell.execute_reply.started": "2025-05-23T17:05:06.128712Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Connect to Roboflow and download the COCO-format segmentation dataset\n",
    "from roboflow import Roboflow\n",
    "import os\n",
    "import json\n",
    "\n",
    "rf = Roboflow(api_key=\"Mvt9FCxE4mY6vBy5OG08\")  # Replace with your key if needed\n",
    "project = rf.workspace(\"urban-lake-wastef\").project(\"another_approach_try\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"coco-segmentation\")\n",
    "\n",
    "# Set paths to the annotation file and images\n",
    "DATA_ROOT = dataset.location\n",
    "TRAIN_JSON = os.path.join(DATA_ROOT, 'train', '_annotations.coco.json')\n",
    "IMG_DIR = os.path.join(DATA_ROOT, 'train')\n",
    "\n",
    "# Define output directories\n",
    "REGION_ROOT = '/content/extracted_regions'\n",
    "SPLIT_ROOT = '/content/split_regions'\n",
    "os.makedirs(REGION_ROOT, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:05:07.750093Z",
     "iopub.status.busy": "2025-05-23T17:05:07.749825Z",
     "iopub.status.idle": "2025-05-23T17:05:36.646183Z",
     "shell.execute_reply": "2025-05-23T17:05:36.645617Z",
     "shell.execute_reply.started": "2025-05-23T17:05:07.750077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Extract foreground objects from COCO masks and save cropped images\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "# Load COCO annotation JSON\n",
    "with open(TRAIN_JSON) as f:\n",
    "    ann_data = json.load(f)\n",
    "\n",
    "# Create a mapping from category ID to name\n",
    "cat_map = {c['id']: c['name'] for c in ann_data['categories']}\n",
    "\n",
    "# Iterate over all annotations to extract and save cropped object regions\n",
    "for ann in ann_data['annotations']:\n",
    "    img_info = next(img for img in ann_data['images'] if img['id'] == ann['image_id'])\n",
    "    img_path = os.path.join(IMG_DIR, img_info['file_name'])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    seg = ann['segmentation']\n",
    "    mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
    "\n",
    "    for poly in seg:\n",
    "        pts = np.array(poly).reshape(-1, 2)\n",
    "        m = Image.new('L', (img_info['width'], img_info['height']), 0)\n",
    "        ImageDraw.Draw(m).polygon([tuple(p) for p in pts], outline=1, fill=1)\n",
    "        mask = np.maximum(mask, np.array(m))\n",
    "\n",
    "    if mask.sum() < 100:  # Skip very small masks\n",
    "        continue\n",
    "\n",
    "    region = np.array(img) * mask[:, :, None]\n",
    "    region_img = Image.fromarray(region)\n",
    "\n",
    "    label = cat_map[ann['category_id']]\n",
    "    out_dir = os.path.join(REGION_ROOT, label)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    base = os.path.splitext(img_info['file_name'])[0]\n",
    "    out_path = os.path.join(out_dir, f\"{base}_{ann['id']}.png\")\n",
    "    region_img.save(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:05:36.647080Z",
     "iopub.status.busy": "2025-05-23T17:05:36.646876Z",
     "iopub.status.idle": "2025-05-23T17:05:38.736623Z",
     "shell.execute_reply": "2025-05-23T17:05:38.735828Z",
     "shell.execute_reply.started": "2025-05-23T17:05:36.647056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Visualize 10 original vs masked (extracted) regions – before training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "\n",
    "def show_before_after_masked(original_dir, masked_dir, split_name, num_samples=10):\n",
    "    \"\"\"\n",
    "    Displays side-by-side original vs masked images from a dataset split.\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Showing {num_samples} {split_name} images: original vs masked\")\n",
    "\n",
    "    classes = sorted(os.listdir(masked_dir))\n",
    "    selected_images = []\n",
    "\n",
    "    while len(selected_images) < num_samples:\n",
    "        chosen_class = random.choice(classes)\n",
    "        class_mask_dir = os.path.join(masked_dir, chosen_class)\n",
    "        if not os.path.isdir(class_mask_dir):\n",
    "            continue\n",
    "        mask_files = os.listdir(class_mask_dir)\n",
    "        if not mask_files:\n",
    "            continue\n",
    "\n",
    "        chosen_file = random.choice(mask_files)\n",
    "        selected_images.append((chosen_class, chosen_file))\n",
    "\n",
    "    for class_name, file_name in selected_images:\n",
    "        masked_path = os.path.join(masked_dir, class_name, file_name)\n",
    "\n",
    "        # Extract original filename base\n",
    "        original_basename_base = \"_\".join(file_name.split(\"_\")[:-1])\n",
    "        possible_exts = [\".jpg\", \".png\"]\n",
    "        original_path = None\n",
    "\n",
    "        for ext in possible_exts:\n",
    "            candidate = os.path.join(original_dir, original_basename_base + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                original_path = candidate\n",
    "                break\n",
    "\n",
    "        if not original_path:\n",
    "            print(f\"⚠️ Original not found for: {original_basename_base}\")\n",
    "            continue\n",
    "\n",
    "        original_img = Image.open(original_path).convert(\"RGB\")\n",
    "        masked_img = Image.open(masked_path).convert(\"RGB\")\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axs[0].imshow(original_img)\n",
    "        axs[0].set_title(\"Original\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(masked_img)\n",
    "        axs[1].set_title(\"Masked (Extracted)\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        fig.suptitle(f\"🟢 Class: {class_name} | 📄 File: {file_name}\", fontsize=13)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ✅ Call this after Step 3\n",
    "show_before_after_masked(\n",
    "    original_dir=os.path.join(DATA_ROOT, \"train\"),\n",
    "    masked_dir=\"/content/extracted_regions\",\n",
    "    split_name=\"Train Set\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:05:38.737708Z",
     "iopub.status.busy": "2025-05-23T17:05:38.737490Z",
     "iopub.status.idle": "2025-05-23T17:05:39.635196Z",
     "shell.execute_reply": "2025-05-23T17:05:39.634306Z",
     "shell.execute_reply.started": "2025-05-23T17:05:38.737689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 4.5: Split dataset into 60% train, 20% val, 20% test\n",
    "\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create split folders\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(SPLIT_ROOT, split), exist_ok=True)\n",
    "\n",
    "# For each class, split its images into train/val/test\n",
    "for class_name in os.listdir(REGION_ROOT):\n",
    "    class_path = os.path.join(REGION_ROOT, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    files = os.listdir(class_path)\n",
    "    train_files, temp_files = train_test_split(files, test_size=0.4, random_state=42)\n",
    "    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
    "\n",
    "    for split, split_files in zip(['train', 'val', 'test'], [train_files, val_files, test_files]):\n",
    "        split_class_dir = os.path.join(SPLIT_ROOT, split, class_name)\n",
    "        os.makedirs(split_class_dir, exist_ok=True)\n",
    "        for f in split_files:\n",
    "            shutil.copy2(os.path.join(class_path, f), os.path.join(split_class_dir, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:05:39.636912Z",
     "iopub.status.busy": "2025-05-23T17:05:39.636215Z",
     "iopub.status.idle": "2025-05-23T17:05:40.973103Z",
     "shell.execute_reply": "2025-05-23T17:05:40.972500Z",
     "shell.execute_reply.started": "2025-05-23T17:05:39.636883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 5: Define a PyTorch Dataset class with augmentation and weighted sampling\n",
    "\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "class WasteRegionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.03),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.label2id = {}\n",
    "        class_counts = defaultdict(int)\n",
    "\n",
    "        # Collect all image paths and assign class indices\n",
    "        classes = sorted(os.listdir(root_dir))\n",
    "        self.label2id = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "        for c in classes:\n",
    "            class_dir = os.path.join(root_dir, c)\n",
    "            for f in os.listdir(class_dir):\n",
    "                self.samples.append((os.path.join(class_dir, f), self.label2id[c]))\n",
    "                class_counts[self.label2id[c]] += 1\n",
    "\n",
    "        # Compute sample weights for balancing\n",
    "        self.sample_weights = [1.0 / class_counts[label] for _, label in self.samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:05:40.974090Z",
     "iopub.status.busy": "2025-05-23T17:05:40.973783Z",
     "iopub.status.idle": "2025-05-23T17:05:42.084292Z",
     "shell.execute_reply": "2025-05-23T17:05:42.083512Z",
     "shell.execute_reply.started": "2025-05-23T17:05:40.974069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Set device (GPU or CPU) and define paths to the dataset splits\n",
    "from timm import create_model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define the root directories for each data split\n",
    "TRAIN_DIR = os.path.join(SPLIT_ROOT, 'train')\n",
    "VAL_DIR   = os.path.join(SPLIT_ROOT, 'val')\n",
    "TEST_DIR  = os.path.join(SPLIT_ROOT, 'test')\n",
    "\n",
    "# Determine the number of classes from folder names in train set\n",
    "num_classes = len(os.listdir(TRAIN_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:05:42.086081Z",
     "iopub.status.busy": "2025-05-23T17:05:42.085164Z",
     "iopub.status.idle": "2025-05-23T17:05:42.097328Z",
     "shell.execute_reply": "2025-05-23T17:05:42.096764Z",
     "shell.execute_reply.started": "2025-05-23T17:05:42.086060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Initialize datasets and DataLoaders with augmentation and weighted sampling for training\n",
    "\n",
    "# Create dataset instances\n",
    "train_ds = WasteRegionDataset(TRAIN_DIR)\n",
    "val_ds   = WasteRegionDataset(VAL_DIR, transform=train_ds.transform)\n",
    "test_ds  = WasteRegionDataset(TEST_DIR, transform=val_ds.transform)\n",
    "\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Apply WeightedRandomSampler to address class imbalance in training data\n",
    "train_sampler = WeightedRandomSampler(train_ds.sample_weights, len(train_ds.sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler)\n",
    "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:05:42.099960Z",
     "iopub.status.busy": "2025-05-23T17:05:42.099752Z",
     "iopub.status.idle": "2025-05-23T17:05:42.222284Z",
     "shell.execute_reply": "2025-05-23T17:05:42.221510Z",
     "shell.execute_reply.started": "2025-05-23T17:05:42.099944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 8: Define training function with mixup, label smoothing, early stopping, weighted loss, and checkpoint saving\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_one_model(model_name, use_mixup=True, label_smooth=0.1, seed=42, patience=15, save_as=None):\n",
    "    set_seed(seed)\n",
    "    model = create_model(model_name, pretrained=True, num_classes=num_classes).to(device)\n",
    "\n",
    "    labels = [label for _, label in train_ds.samples]\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smooth)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-3)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=3 * len(train_loader),\n",
    "        num_training_steps=len(train_loader) * 100\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    def mixup_data(x, y, alpha=0.4):\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        index = torch.randperm(x.size(0)).to(x.device)\n",
    "        mixed_x = lam * x + (1 - lam) * x[index]\n",
    "        y_a, y_b = y, y[index]\n",
    "        return mixed_x, y_a, y_b, lam\n",
    "\n",
    "    def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "    save_path = f\"{save_as}_best.pth\" if save_as else f\"{model_name}_best.pth\"\n",
    "\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_mixup:\n",
    "                imgs, y_a, y_b, lam = mixup_data(imgs, labels)\n",
    "            else:\n",
    "                y_a, y_b, lam = labels, labels, 1.0\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(imgs)\n",
    "                loss = mixup_criterion(criterion, outputs, y_a, y_b, lam) if use_mixup else criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"[{model_name}] Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improve_epochs = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"🔸 New best model saved.\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= patience:\n",
    "                print(\"🛑 Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T17:05:42.223362Z",
     "iopub.status.busy": "2025-05-23T17:05:42.223002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 9: Train multiple configurations of model and save each model uniquely\n",
    "import csv\n",
    "\n",
    "model_list = ['convnext_tiny']\n",
    "seeds = [42, 123, 777]\n",
    "mixup_options = [True, False]\n",
    "label_smoothings = [0.1, 0.0]\n",
    "\n",
    "all_histories = {}\n",
    "\n",
    "with open(\"ablation_results.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Seed\", \"Mixup\", \"Label Smoothing\", \"Val Accuracy\", \"Checkpoint\"])\n",
    "\n",
    "    for model_name in model_list:\n",
    "        for mixup in mixup_options:\n",
    "            for smooth in label_smoothings:\n",
    "                for seed in seeds:\n",
    "                    run_id = f\"{model_name}_mixup{mixup}_smooth{smooth}_seed{seed}\"\n",
    "                    print(f\"🔁 Training: {run_id}\")\n",
    "\n",
    "                    model, history = train_one_model(\n",
    "                        model_name=model_name,\n",
    "                        use_mixup=mixup,\n",
    "                        label_smooth=smooth,\n",
    "                        seed=seed,\n",
    "                        patience=15,\n",
    "                        save_as=run_id\n",
    "                    )\n",
    "\n",
    "                    final_val_acc = history['val_acc'][-1]\n",
    "                    writer.writerow([model_name, seed, mixup, smooth, final_val_acc, f\"{run_id}_best.pth\"])\n",
    "                    all_histories[run_id] = history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step X: Install and import Grad-CAM + helper functions\n",
    "!pip install -q grad-cam\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from torchvision.transforms.functional import resize\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Automatically detect the target layer for Grad-CAM\n",
    "def get_gradcam_target_layer(model):\n",
    "    if hasattr(model, \"layer4\"):\n",
    "        return [model.layer4[-1]]\n",
    "    if hasattr(model, \"blocks\"):\n",
    "        return [model.blocks[-1]]\n",
    "    if hasattr(model, \"features\"):\n",
    "        return [model.features[-1]]\n",
    "    if hasattr(model, \"stages\"):\n",
    "        return [model.stages[-1][-1]]\n",
    "    print(\"❗ Unable to automatically determine target layer.\")\n",
    "    for name, module in model.named_modules():\n",
    "        print(name)\n",
    "    raise ValueError(\"Set target_layers manually.\")\n",
    "\n",
    "# Visualize Grad-CAM for a given image and model\n",
    "def visualize_gradcam(img_path, model, transform, class_id=None):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    rgb_img = np.array(resize(img, (224, 224))).astype(np.float32) / 255\n",
    "\n",
    "    target_layers = get_gradcam_target_layer(model)\n",
    "    targets = [ClassifierOutputTarget(class_id)] if class_id is not None else None\n",
    "\n",
    "    # Grad-CAM with context manager to suppress warnings\n",
    "    with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    plt.imshow(cam_image)\n",
    "    plt.title(\"Grad-CAM Visualization\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 10: Plot training vs validation loss and validation accuracy to assess learning and overfitting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_histories(all_histories):\n",
    "    \"\"\"\n",
    "    Plots loss and accuracy curves from the training history of each model configuration.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_histories (dict): Dictionary of training histories returned by train_one_model().\n",
    "                            Keys are run names, values are dicts with 'train_loss', 'val_loss', 'val_acc'.\n",
    "\n",
    "    Each model will generate:\n",
    "    - Train vs Val Loss curve\n",
    "    - Val Accuracy curve\n",
    "    \"\"\"\n",
    "    for run_id, hist in all_histories.items():\n",
    "        epochs = range(1, len(hist['train_loss']) + 1)\n",
    "        \n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        # Loss Curve\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, hist['train_loss'], label='Train Loss', marker='o')\n",
    "        plt.plot(epochs, hist['val_loss'], label='Val Loss', marker='x')\n",
    "        plt.title(f\"{run_id} – Loss Curve\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Accuracy Curve\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, hist['val_acc'], label='Val Accuracy', marker='s', color='green')\n",
    "        plt.title(f\"{run_id} – Accuracy Curve\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.suptitle(f\"Training Progress – {run_id}\", fontsize=14)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 11: Generate overfitting plots for all trained models using saved training histories\n",
    "plot_model_histories(all_histories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 12: Read the CSV results and compute average and std deviation for each model setting\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ablation_results.csv\")\n",
    "\n",
    "summary = df.groupby([\"Model\", \"Mixup\", \"Label Smoothing\"])[\"Val Accuracy\"].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "print(\"📊 Ablation Results (mean ± std across seeds):\")\n",
    "print(summary)\n",
    "\n",
    "# 🔍 Select best run\n",
    "best_row = df.loc[df[\"Val Accuracy\"].idxmax()]\n",
    "print(\"\\n\\n🏆 Best configuration:\\\\n\", best_row)\n",
    "\n",
    "# Extract values for Step 11\n",
    "best_checkpoint_path = best_row[\"Checkpoint\"]\n",
    "best_model_name = best_row[\"Model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 13: Evaluate Best-Performing Model on Held-Out Test Set\n",
    "# ============================================================\n",
    "# This step loads the best model configuration (based on validation accuracy)\n",
    "# and evaluates it on the test dataset using accuracy, precision, recall,\n",
    "# F1 score, confusion matrix, and ROC AUC. All model parameters used in training\n",
    "# (architecture, mixup, label smoothing, seed) are printed for traceability.\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# 🔎 Show model configuration\n",
    "print(\"🧪 STEP 13: Evaluating Best Model Configuration on TEST SET\")\n",
    "print(f\"Model Architecture: {best_row['Model']}\")\n",
    "print(f\"Mixup Enabled:      {best_row['Mixup']}\")\n",
    "print(f\"Label Smoothing:    {best_row['Label Smoothing']}\")\n",
    "print(f\"Random Seed:        {best_row['Seed']}\")\n",
    "print(f\"Validation Accuracy:{best_row['Val Accuracy']:.4f}\")\n",
    "print(f\"Checkpoint File:    {best_row['Checkpoint']}\")\n",
    "print(f\"➡️ Dataset Used:      Held-out TEST SET\\n\")\n",
    "\n",
    "# 🔧 Load best-performing model\n",
    "model = create_model(best_model_name, pretrained=False, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(best_checkpoint_path))\n",
    "\n",
    "def evaluate_model_detailed(model, dataloader, device, label2id, show_roc_auc=True):\n",
    "    model.eval()\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    target_names = [id2label[i] for i in sorted(id2label)]\n",
    "\n",
    "    print(\"🔍 Classification Report (on Test Set):\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "    print(f\"Test Accuracy:  {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "    print(f\"Test Precision: {precision_score(all_labels, all_preds, average='weighted'):.4f}\")\n",
    "    print(f\"Test Recall:    {recall_score(all_labels, all_preds, average='weighted'):.4f}\")\n",
    "    print(f\"Test F1-Score:  {f1_score(all_labels, all_preds, average='weighted'):.4f}\")\n",
    "\n",
    "    # Confusion Matrix – Raw\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(\"Confusion Matrix – Raw (Test Set)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion Matrix – Normalized\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Oranges',\n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(\"Confusion Matrix – Normalized (Test Set)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC AUC Score\n",
    "    if show_roc_auc:\n",
    "        try:\n",
    "            y_true_bin = label_binarize(all_labels, classes=list(range(len(label2id))))\n",
    "            auc = roc_auc_score(y_true_bin, all_probs, multi_class='ovr')\n",
    "            print(f\"Test ROC AUC (Multiclass OVR): {auc:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ ROC-AUC computation failed: {e}\")\n",
    "\n",
    "# 🧪 Evaluate best model on test data\n",
    "evaluate_model_detailed(model, test_loader, device, train_ds.label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def get_gradcam_target_layer(model):\n",
    "    if hasattr(model, \"layer4\"):\n",
    "        return [model.layer4[-1]]\n",
    "    if hasattr(model, \"blocks\"):\n",
    "        return [model.blocks[-1]]\n",
    "    if hasattr(model, \"features\"):\n",
    "        return [model.features[-1]]\n",
    "    if hasattr(model, \"stages\"):\n",
    "        return [model.stages[-1][-1]]\n",
    "    for name, module in model.named_modules():\n",
    "        print(name)\n",
    "    raise ValueError(\"Set target_layers manually.\")\n",
    "\n",
    "def gradcam_comparison(model, img_path, transform, true_label, class_names=None, save_dir=None):\n",
    "    \"\"\"\n",
    "    Generates and displays a side-by-side view of the original image and Grad-CAM overlay.\n",
    "    Optionally saves the output to disk.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    rgb_img = np.array(resize(img, (224, 224))).astype(np.float32) / 255\n",
    "\n",
    "    # Predict class\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "    # Grad-CAM\n",
    "    target_layers = get_gradcam_target_layer(model)\n",
    "    targets = [ClassifierOutputTarget(pred_class)]\n",
    "\n",
    "    with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "    cam_overlay = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    # Display\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(cam_overlay)\n",
    "    axs[1].set_title(\"Grad-CAM Heatmap\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    label_display = f\"✅ True: {true_label} | 🔮 Predicted: {class_names[pred_class] if class_names else pred_class}\"\n",
    "    fig.suptitle(label_display, fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save if directory is specified\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = os.path.basename(img_path).split('.')[0]\n",
    "        fig.savefig(os.path.join(save_dir, f\"{filename}_gradcam.png\"))\n",
    "        print(f\"📁 Saved to: {os.path.join(save_dir, f'{filename}_gradcam.png')}\")\n",
    "    plt.close()\n",
    "\n",
    "# === Step 14: Run Grad-CAM on one random image per class ===\n",
    "\n",
    "# Load trained model\n",
    "model = create_model(best_model_name, pretrained=False, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(best_checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Get class names\n",
    "class_names = sorted(os.listdir(TRAIN_DIR))\n",
    "\n",
    "# Output folder\n",
    "save_dir = \"/kaggle/working/Another_approach_try-4\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# One random image per class\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(TEST_DIR, class_name)\n",
    "    img_files = os.listdir(class_path)\n",
    "    if not img_files:\n",
    "        continue\n",
    "\n",
    "    img_name = random.choice(img_files)\n",
    "    img_path = os.path.join(class_path, img_name)\n",
    "    print(f\"\\n📷 Selected for class '{class_name}': {img_name}\")\n",
    "\n",
    "    gradcam_comparison(\n",
    "        model=model,\n",
    "        img_path=img_path,\n",
    "        transform=test_ds.transform,\n",
    "        true_label=class_name,\n",
    "        class_names=class_names,\n",
    "        save_dir=save_dir\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
